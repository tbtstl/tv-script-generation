{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TV Script Generation\n",
    "In this notebook, we'll construct an RNN model to generate television scripts for a TV show. \n",
    "\n",
    "The notebook implements some ideas found from [this /r/machinelearning post](https://www.reddit.com/r/MachineLearning/comments/3psqil/sentence_to_sentence_text_generation_using_lstms/), [this paper on applying dropout to LSTM units](https://arxiv.org/pdf/1409.2329.pdf), and [this paper on output embedding for language models](https://arxiv.org/pdf/1409.2329.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "GPU_AVAILABLE = torch.cuda.is_available()\n",
    "GPU_AVAILABLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "To start, we can use the [The Simpsons by the Data](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data) dataset from Kaggle.\n",
    "\n",
    "The tv scripts are originally in .csv format, but since we're only going to train our model on the raw text of the scripts, I've converted the .csv into a simple text file, [here](./data/simpsons/simpsons_script_lines.txt). \n",
    "\n",
    "I've also split the original dataset into 50% [training](./data/simpsons/train.txt), 25% [testing](./data/simpsons/test.txt), and 25% [validation](valid.txt) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marge Simpson: Yes.\n",
      "Lisa Simpson: Can we do it this week?\n",
      "(Springfield Elementary School: INT. ELEMENTARY - HALLWAY)\n",
      "Lisa Simpson: (REHEARSING) Mr. Bergstrom, we request the pleasure of your company... no... Mr. Bergstrom, if you're not doing anything this Friday... no... Mr. Bergstrom, do you like pork chops... oh no, of course you wouldn't...\n",
      "Miss Hoover: Good morning, Lisa.\n",
      "Miss Hoover: (OFF LISA'S REACTION) I'm back.\n",
      "Miss Hoover: You see, class, my Lyme disease turned out to be...\n",
      "Miss Hoover: Psy-cho-so-ma-tic.\n",
      "Ralph Wiggum: Does that mean you were crazy?\n",
      "JANEY: No, that means she was faking it."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tail ./data/simpsons/simpsons_script_lines.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the snippet above, there are three instances of the word `no`. Unfortunately, when we start adding words to our dictionary, `no...` and `no,` will be treated as entirely different words. To remedy this, we can create a token lookup function to map special characters to their own words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_lookup():\n",
    "    return {\n",
    "        '.': '||period||',\n",
    "        ',': '||comma||',\n",
    "        '\"': '||quotation_mark||',\n",
    "        ';': '||semicolon||',\n",
    "        '!': '||exclamation_mark',\n",
    "        '?': '||question_mark',\n",
    "        '(': '||l_parantheses||',\n",
    "        ')': '||r_parantheses||',\n",
    "        '-': '||dash||',\n",
    "        '\\'': '||apostrophe||',\n",
    "        '\\n': '||return||'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary Class\n",
    "The Dictionary can hold our text encodings for later referral. It will also have a helper function to easily query the number of words and to add words to the dictionary instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = []\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word_to_idx:\n",
    "            self.idx_to_word.append(word)\n",
    "            self.word_to_idx[word] = len(self.idx_to_word) -1\n",
    "        return self.word_to_idx[word]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Class\n",
    "The Corpus can contain the training, testing, and validation sets for the model, as well as a dictionary of words. The corpus is capable of tokenizing its data to help with punctuation and capitalization. Note that the tokenization adds delimeters around the tokenized punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
    "        \n",
    "    def tokenize(self, path):\n",
    "        assert os.path.exists(path), \"Could not find file matching %s\" % path\n",
    "        \n",
    "        # Add words to the diictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                for key, tok in token_lookup().items():\n",
    "                    line = line.replace(key, ' {} '.format(tok))\n",
    "                line = line.lower()\n",
    "                \n",
    "                words = line.split() + ['||return||']\n",
    "                tokens += len(words)\n",
    "                \n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "        \n",
    "        # Tokenize the words and return a LongTensor of ids\n",
    "        with open(path, 'r') as f:\n",
    "            ids = torch.LongTensor(tokens)\n",
    "            token = 0\n",
    "            for line in f:\n",
    "                for key, tok in token_lookup().items():\n",
    "                    line = line.replace(key, ' {} '.format(tok))\n",
    "                line = line.lower()\n",
    "                words = line.split() + ['||return||']\n",
    "                \n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word_to_idx[word]\n",
    "                    token += 1\n",
    "        return ids            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "With those two classes, we can actually load our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pathname = './data/simpsons/'\n",
    "corpus = Corpus(pathname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch the data\n",
    "We need to split up our data into batches for training. We can do so with a batchify method like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchify(data, batch_size):\n",
    "    # Work out how cleanly we can divide the dataset into :batch_size: parts\n",
    "    n_batches = data.size(0) // batch_size\n",
    "    \n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders)\n",
    "    data = data.narrow(0, 0, n_batches * batch_size)\n",
    "    \n",
    "    # Evenly divide the data across the :batch_size: batches.\n",
    "    data = data.view(batch_size, -1).t().contiguous()\n",
    "    \n",
    "    if GPU_AVAILABLE:\n",
    "        data = data.cuda()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluation_batch_size = 10\n",
    "batch_size = 20\n",
    "train_data = batchify(corpus.train, batch_size)\n",
    "test_data = batchify(corpus.test, evaluation_batch_size)\n",
    "validation_data = batchify(corpus.valid, evaluation_batch_size)\n",
    "n_tokens = len(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, n_tokens, n_inputs, n_hidden, n_layers, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        # Randomly zeroes some of the elements of\n",
    "        #  the input tensor with probability :dropout:\n",
    "        #  using samples from a bernoulli distribution\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # A simple lookup table that stores embeddings \n",
    "        #  of a fixed dictionary and size.\n",
    "        # We are using it to store word embeddings\n",
    "        self.encoder = nn.Embedding(n_tokens, n_inputs)\n",
    "        \n",
    "        # A multi-layer long short-term memory cell\n",
    "        self.rnn = nn.LSTM(n_inputs, n_hidden, n_layers, dropout=dropout)\n",
    "        \n",
    "        # A layer to apply a linear transformation to the incoming data\n",
    "        self.decoder = nn.Linear(n_hidden, n_tokens)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "    def init_weights(self):\n",
    "        # Initialize the weights of our encoder and decoder with random \n",
    "        #  values sampled from a uniform distribution over (-0.1, 0.1).\n",
    "        # Also set the bias of our linear transformation to 0\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        # At every call, run the input through here\n",
    "        embedding = self.dropout(self.encoder(input))\n",
    "        output, hidden = self.rnn(embedding, hidden)\n",
    "        output = self.dropout(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize the hidden layer with zeros\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        return (\n",
    "            Variable(weight.new(self.n_layers, batch_size, self.n_hidden).zero_()),\n",
    "            Variable(weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 1500\n",
    "num_hidden = 650\n",
    "dropout_probability = 0.65\n",
    "num_epochs = 50\n",
    "num_layers = 2\n",
    "num_tokens = len(corpus.dictionary)\n",
    "sequence_length = 15\n",
    "learning_rate = 10.0\n",
    "gradient_clipping = 0.3\n",
    "logging_interval = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RNNModel(num_tokens, embedding_size, num_hidden, num_layers, dropout_probability)\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our Criterion\n",
    "We will calculate loss based off of PyTorch's `nn.CrossEntropyLoss`. This criterion performs a `Log(SoftMax(x))` function to a tensor, and passes the result to a negative log likelihood function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`repackage_hidden_states` detaches variables from their history to avoid backpropogating through the entire training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def repackage_hidden_states(h):\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden_states(v) for v in h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_batch` gets a batch to use in each training or evaluation pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(source, i, evaluation=False):\n",
    "    sequence_len = min(sequence_length, len(source)-1-i)\n",
    "    data = Variable(source[i:i+sequence_len], volatile=evaluation)\n",
    "    target = Variable(source[i+1:i+1+sequence_len].view(-1))\n",
    "    \n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`evaluate` Turns on evaluation mode, which disables dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(data_source):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    ntok = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(evaluation_batch_size)\n",
    "    \n",
    "    for i in range(0, data_source.size(0) -1, sequence_length):\n",
    "        data, targets = get_batch(data_source, i, evaluation=True)\n",
    "        output, hidden = model(data, hidden)\n",
    "        output_flat = output.view(-1, ntok)\n",
    "        total_loss += len(data) * criterion(output_flat, targets).data\n",
    "        hidden = repackage_hidden_states(hidden)\n",
    "    return total_loss[0] / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train():\n",
    "    lr = learning_rate\n",
    "    best_validation_loss = None\n",
    "#     validation_losses = np.zeros(num_epochs)\n",
    "#     learning_rates = np.zeros(num_epochs)\n",
    "#     print(validation_losses)\n",
    "#     print(learning_rates)\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        n_tokens = len(corpus.dictionary)\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch, i in enumerate(range(0, train_data.size(0) - 1, sequence_length)):\n",
    "            data, targets = get_batch(train_data, i)\n",
    "\n",
    "            hidden = repackage_hidden_states(hidden)\n",
    "            model.zero_grad()\n",
    "            output, hidden = model(data, hidden)\n",
    "            loss = criterion(output.view(-1, n_tokens), targets)\n",
    "            loss.backward()\n",
    "\n",
    "            # clip_grad_norm prevents the exploding gradient problem in RNNs and LSTMs\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), gradient_clipping)\n",
    "\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "            total_loss += loss.data\n",
    "\n",
    "            if batch % logging_interval == 0 and batch > 0:\n",
    "                current_loss = total_loss[0] / logging_interval\n",
    "                elapsed = time.time() - start_time\n",
    "                print('Epoch {:3d} {:5d}/{:5d} batches: lr {:02.2f}, loss {:5.2f}, perplexity: {:8.2f}'.format(\n",
    "                        epoch, batch, len(train_data) // sequence_length, lr, current_loss, math.exp(current_loss))\n",
    "                     )\n",
    "                total_loss = 0\n",
    "        validation_loss = evaluate(validation_data)\n",
    "        \n",
    "#         validation_losses = np.put(validation_losses, epoch+1, validation_loss)\n",
    "#         learning_rates = np.put(learning_rates, epoch+1, lr)\n",
    "        \n",
    "#         fig = plt.figure()\n",
    "#         ax = plt.subplot(111)\n",
    "#         print(learning_rates)\n",
    "#         print(np.arange(1, num_epochs+1))\n",
    "#         ax.plot(np.arange(1, num_epochs+1), learning_rates, label='Learning Rate')\n",
    "#         ax.plot(np.arange(1, num_epochs+1), validation_losses, label='Validation Loss')\n",
    "#         ax.legend()\n",
    "#         plt.show()\n",
    "    \n",
    "        \n",
    "        \n",
    "        print('='*76)\n",
    "        print('Epoch {:3d} results: time: {:5.2f}s, validation loss {:5.2f}, perplexity {:8.2f}'.format(\n",
    "            epoch, (time.time() - epoch_start_time), validation_loss, math.exp(validation_loss)\n",
    "        ))\n",
    "        print('='*76)\n",
    "\n",
    "        \n",
    "        # If the model's validation loss is the best we've seen, save the model\n",
    "        if not best_validation_loss or validation_loss < best_validation_loss:\n",
    "            with open('model_checkpoint.pt', 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_validation_loss = validation_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset\n",
    "            lr /= 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1   250/ 6203 batches: lr 10.00, loss  6.41, perplexity:   608.77\n",
      "Epoch   1   500/ 6203 batches: lr 10.00, loss  5.05, perplexity:   156.34\n",
      "Epoch   1   750/ 6203 batches: lr 10.00, loss  4.74, perplexity:   114.48\n",
      "Epoch   1  1000/ 6203 batches: lr 10.00, loss  4.53, perplexity:    92.61\n",
      "Epoch   1  1250/ 6203 batches: lr 10.00, loss  4.44, perplexity:    84.88\n",
      "Epoch   1  1500/ 6203 batches: lr 10.00, loss  4.39, perplexity:    80.91\n",
      "Epoch   1  1750/ 6203 batches: lr 10.00, loss  4.42, perplexity:    82.88\n",
      "Epoch   1  2000/ 6203 batches: lr 10.00, loss  4.36, perplexity:    78.38\n",
      "Epoch   1  2250/ 6203 batches: lr 10.00, loss  4.32, perplexity:    74.83\n",
      "Epoch   1  2500/ 6203 batches: lr 10.00, loss  4.22, perplexity:    68.10\n",
      "Epoch   1  2750/ 6203 batches: lr 10.00, loss  4.25, perplexity:    69.90\n",
      "Epoch   1  3000/ 6203 batches: lr 10.00, loss  4.27, perplexity:    71.78\n",
      "Epoch   1  3250/ 6203 batches: lr 10.00, loss  4.16, perplexity:    64.11\n",
      "Epoch   1  3500/ 6203 batches: lr 10.00, loss  4.11, perplexity:    61.12\n",
      "Epoch   1  3750/ 6203 batches: lr 10.00, loss  4.15, perplexity:    63.72\n",
      "Epoch   1  4000/ 6203 batches: lr 10.00, loss  4.12, perplexity:    61.57\n",
      "Epoch   1  4250/ 6203 batches: lr 10.00, loss  4.08, perplexity:    59.23\n",
      "Epoch   1  4500/ 6203 batches: lr 10.00, loss  4.05, perplexity:    57.17\n",
      "Epoch   1  4750/ 6203 batches: lr 10.00, loss  4.05, perplexity:    57.43\n",
      "Epoch   1  5000/ 6203 batches: lr 10.00, loss  4.00, perplexity:    54.87\n",
      "Epoch   1  5250/ 6203 batches: lr 10.00, loss  4.02, perplexity:    55.78\n",
      "Epoch   1  5500/ 6203 batches: lr 10.00, loss  4.03, perplexity:    56.16\n",
      "Epoch   1  5750/ 6203 batches: lr 10.00, loss  4.06, perplexity:    58.05\n",
      "Epoch   1  6000/ 6203 batches: lr 10.00, loss  4.04, perplexity:    56.60\n",
      "============================================================================\n",
      "Epoch   1 results: time: 454.75s, validation loss  3.99, perplexity    53.81\n",
      "============================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyson/anaconda3/envs/ml/lib/python3.6/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type RNNModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2   250/ 6203 batches: lr 10.00, loss  3.97, perplexity:    52.92\n",
      "Epoch   2   500/ 6203 batches: lr 10.00, loss  3.93, perplexity:    51.16\n",
      "Epoch   2   750/ 6203 batches: lr 10.00, loss  3.92, perplexity:    50.56\n",
      "Epoch   2  1000/ 6203 batches: lr 10.00, loss  3.85, perplexity:    47.03\n",
      "Epoch   2  1250/ 6203 batches: lr 10.00, loss  3.87, perplexity:    47.75\n",
      "Epoch   2  1500/ 6203 batches: lr 10.00, loss  3.89, perplexity:    48.82\n",
      "Epoch   2  1750/ 6203 batches: lr 10.00, loss  3.94, perplexity:    51.54\n",
      "Epoch   2  2000/ 6203 batches: lr 10.00, loss  3.94, perplexity:    51.48\n",
      "Epoch   2  2250/ 6203 batches: lr 10.00, loss  3.92, perplexity:    50.51\n",
      "Epoch   2  2500/ 6203 batches: lr 10.00, loss  3.86, perplexity:    47.57\n",
      "Epoch   2  2750/ 6203 batches: lr 10.00, loss  3.90, perplexity:    49.44\n",
      "Epoch   2  3000/ 6203 batches: lr 10.00, loss  3.95, perplexity:    51.89\n",
      "Epoch   2  3250/ 6203 batches: lr 10.00, loss  3.86, perplexity:    47.28\n",
      "Epoch   2  3500/ 6203 batches: lr 10.00, loss  3.82, perplexity:    45.71\n",
      "Epoch   2  3750/ 6203 batches: lr 10.00, loss  3.89, perplexity:    48.81\n",
      "Epoch   2  4000/ 6203 batches: lr 10.00, loss  3.86, perplexity:    47.30\n",
      "Epoch   2  4250/ 6203 batches: lr 10.00, loss  3.82, perplexity:    45.68\n",
      "Epoch   2  4500/ 6203 batches: lr 10.00, loss  3.80, perplexity:    44.84\n",
      "Epoch   2  4750/ 6203 batches: lr 10.00, loss  3.83, perplexity:    45.84\n",
      "Epoch   2  5000/ 6203 batches: lr 10.00, loss  3.78, perplexity:    43.96\n",
      "Epoch   2  5250/ 6203 batches: lr 10.00, loss  3.81, perplexity:    45.23\n",
      "Epoch   2  5500/ 6203 batches: lr 10.00, loss  3.82, perplexity:    45.78\n",
      "Epoch   2  5750/ 6203 batches: lr 10.00, loss  3.86, perplexity:    47.32\n",
      "Epoch   2  6000/ 6203 batches: lr 10.00, loss  3.84, perplexity:    46.41\n",
      "============================================================================\n",
      "Epoch   2 results: time: 455.06s, validation loss  3.84, perplexity    46.33\n",
      "============================================================================\n",
      "Epoch   3   250/ 6203 batches: lr 10.00, loss  3.79, perplexity:    44.31\n",
      "Epoch   3   500/ 6203 batches: lr 10.00, loss  3.77, perplexity:    43.26\n",
      "Epoch   3   750/ 6203 batches: lr 10.00, loss  3.77, perplexity:    43.37\n",
      "Epoch   3  1000/ 6203 batches: lr 10.00, loss  3.69, perplexity:    40.16\n",
      "Epoch   3  1250/ 6203 batches: lr 10.00, loss  3.73, perplexity:    41.48\n",
      "Epoch   3  1500/ 6203 batches: lr 10.00, loss  3.75, perplexity:    42.52\n",
      "Epoch   3  1750/ 6203 batches: lr 10.00, loss  3.80, perplexity:    44.58\n",
      "Epoch   3  2000/ 6203 batches: lr 10.00, loss  3.81, perplexity:    45.01\n",
      "Epoch   3  2250/ 6203 batches: lr 10.00, loss  3.79, perplexity:    44.41\n",
      "Epoch   3  2500/ 6203 batches: lr 10.00, loss  3.73, perplexity:    41.74\n",
      "Epoch   3  2750/ 6203 batches: lr 10.00, loss  3.77, perplexity:    43.47\n",
      "Epoch   3  3000/ 6203 batches: lr 10.00, loss  3.83, perplexity:    45.94\n",
      "Epoch   3  3250/ 6203 batches: lr 10.00, loss  3.73, perplexity:    41.86\n",
      "Epoch   3  3500/ 6203 batches: lr 10.00, loss  3.71, perplexity:    40.74\n",
      "Epoch   3  3750/ 6203 batches: lr 10.00, loss  3.77, perplexity:    43.52\n",
      "Epoch   3  4000/ 6203 batches: lr 10.00, loss  3.75, perplexity:    42.42\n",
      "Epoch   3  4250/ 6203 batches: lr 10.00, loss  3.71, perplexity:    40.93\n",
      "Epoch   3  4500/ 6203 batches: lr 10.00, loss  3.70, perplexity:    40.42\n",
      "Epoch   3  4750/ 6203 batches: lr 10.00, loss  3.72, perplexity:    41.38\n",
      "Epoch   3  5000/ 6203 batches: lr 10.00, loss  3.69, perplexity:    39.92\n",
      "Epoch   3  5250/ 6203 batches: lr 10.00, loss  3.71, perplexity:    40.93\n",
      "Epoch   3  5500/ 6203 batches: lr 10.00, loss  3.72, perplexity:    41.42\n",
      "Epoch   3  5750/ 6203 batches: lr 10.00, loss  3.75, perplexity:    42.59\n",
      "Epoch   3  6000/ 6203 batches: lr 10.00, loss  3.74, perplexity:    42.11\n",
      "============================================================================\n",
      "Epoch   3 results: time: 453.29s, validation loss  3.78, perplexity    43.71\n",
      "============================================================================\n",
      "Epoch   4   250/ 6203 batches: lr 10.00, loss  3.70, perplexity:    40.41\n",
      "Epoch   4   500/ 6203 batches: lr 10.00, loss  3.69, perplexity:    39.97\n",
      "Epoch   4   750/ 6203 batches: lr 10.00, loss  3.69, perplexity:    39.88\n",
      "Epoch   4  1000/ 6203 batches: lr 10.00, loss  3.61, perplexity:    36.99\n",
      "Epoch   4  1250/ 6203 batches: lr 10.00, loss  3.65, perplexity:    38.38\n",
      "Epoch   4  1500/ 6203 batches: lr 10.00, loss  3.67, perplexity:    39.24\n",
      "Epoch   4  1750/ 6203 batches: lr 10.00, loss  3.72, perplexity:    41.10\n",
      "Epoch   4  2000/ 6203 batches: lr 10.00, loss  3.72, perplexity:    41.37\n",
      "Epoch   4  2250/ 6203 batches: lr 10.00, loss  3.72, perplexity:    41.11\n",
      "Epoch   4  2500/ 6203 batches: lr 10.00, loss  3.65, perplexity:    38.57\n",
      "Epoch   4  2750/ 6203 batches: lr 10.00, loss  3.69, perplexity:    40.14\n",
      "Epoch   4  3000/ 6203 batches: lr 10.00, loss  3.75, perplexity:    42.65\n",
      "Epoch   4  3250/ 6203 batches: lr 10.00, loss  3.66, perplexity:    38.90\n",
      "Epoch   4  3500/ 6203 batches: lr 10.00, loss  3.64, perplexity:    38.09\n",
      "Epoch   4  3750/ 6203 batches: lr 10.00, loss  3.70, perplexity:    40.61\n",
      "Epoch   4  4000/ 6203 batches: lr 10.00, loss  3.67, perplexity:    39.42\n",
      "Epoch   4  4250/ 6203 batches: lr 10.00, loss  3.64, perplexity:    38.12\n",
      "Epoch   4  4500/ 6203 batches: lr 10.00, loss  3.63, perplexity:    37.72\n",
      "Epoch   4  4750/ 6203 batches: lr 10.00, loss  3.66, perplexity:    38.76\n",
      "Epoch   4  5000/ 6203 batches: lr 10.00, loss  3.62, perplexity:    37.33\n",
      "Epoch   4  5250/ 6203 batches: lr 10.00, loss  3.65, perplexity:    38.48\n",
      "Epoch   4  5500/ 6203 batches: lr 10.00, loss  3.66, perplexity:    38.78\n",
      "Epoch   4  5750/ 6203 batches: lr 10.00, loss  3.69, perplexity:    39.91\n",
      "Epoch   4  6000/ 6203 batches: lr 10.00, loss  3.68, perplexity:    39.46\n",
      "============================================================================\n",
      "Epoch   4 results: time: 453.18s, validation loss  3.74, perplexity    41.96\n",
      "============================================================================\n",
      "Epoch   5   250/ 6203 batches: lr 10.00, loss  3.64, perplexity:    38.21\n",
      "Epoch   5   500/ 6203 batches: lr 10.00, loss  3.62, perplexity:    37.48\n",
      "Epoch   5   750/ 6203 batches: lr 10.00, loss  3.63, perplexity:    37.59\n",
      "Epoch   5  1000/ 6203 batches: lr 10.00, loss  3.55, perplexity:    34.94\n",
      "Epoch   5  1250/ 6203 batches: lr 10.00, loss  3.59, perplexity:    36.33\n",
      "Epoch   5  1500/ 6203 batches: lr 10.00, loss  3.61, perplexity:    36.98\n",
      "Epoch   5  1750/ 6203 batches: lr 10.00, loss  3.66, perplexity:    38.97\n",
      "Epoch   5  2000/ 6203 batches: lr 10.00, loss  3.67, perplexity:    39.10\n",
      "Epoch   5  2250/ 6203 batches: lr 10.00, loss  3.66, perplexity:    38.75\n",
      "Epoch   5  2500/ 6203 batches: lr 10.00, loss  3.60, perplexity:    36.65\n",
      "Epoch   5  2750/ 6203 batches: lr 10.00, loss  3.64, perplexity:    38.22\n",
      "Epoch   5  3000/ 6203 batches: lr 10.00, loss  3.70, perplexity:    40.30\n",
      "Epoch   5  3250/ 6203 batches: lr 10.00, loss  3.61, perplexity:    36.92\n",
      "Epoch   5  3500/ 6203 batches: lr 10.00, loss  3.59, perplexity:    36.19\n",
      "Epoch   5  3750/ 6203 batches: lr 10.00, loss  3.65, perplexity:    38.57\n",
      "Epoch   5  4000/ 6203 batches: lr 10.00, loss  3.63, perplexity:    37.56\n",
      "Epoch   5  4250/ 6203 batches: lr 10.00, loss  3.59, perplexity:    36.27\n",
      "Epoch   5  4500/ 6203 batches: lr 10.00, loss  3.58, perplexity:    35.83\n",
      "Epoch   5  4750/ 6203 batches: lr 10.00, loss  3.61, perplexity:    37.05\n",
      "Epoch   5  5000/ 6203 batches: lr 10.00, loss  3.57, perplexity:    35.64\n",
      "Epoch   5  5250/ 6203 batches: lr 10.00, loss  3.60, perplexity:    36.57\n",
      "Epoch   5  5500/ 6203 batches: lr 10.00, loss  3.61, perplexity:    36.89\n",
      "Epoch   5  5750/ 6203 batches: lr 10.00, loss  3.64, perplexity:    38.19\n",
      "Epoch   5  6000/ 6203 batches: lr 10.00, loss  3.62, perplexity:    37.41\n",
      "============================================================================\n",
      "Epoch   5 results: time: 453.31s, validation loss  3.71, perplexity    40.98\n",
      "============================================================================\n",
      "Epoch   6   250/ 6203 batches: lr 10.00, loss  3.59, perplexity:    36.33\n",
      "Epoch   6   500/ 6203 batches: lr 10.00, loss  3.59, perplexity:    36.08\n",
      "Epoch   6   750/ 6203 batches: lr 10.00, loss  3.58, perplexity:    36.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   6  1000/ 6203 batches: lr 10.00, loss  3.52, perplexity:    33.76\n",
      "Epoch   6  1250/ 6203 batches: lr 10.00, loss  3.55, perplexity:    34.98\n",
      "Epoch   6  1500/ 6203 batches: lr 10.00, loss  3.56, perplexity:    35.29\n",
      "Epoch   6  1750/ 6203 batches: lr 10.00, loss  3.62, perplexity:    37.27\n",
      "Epoch   6  2000/ 6203 batches: lr 10.00, loss  3.63, perplexity:    37.72\n",
      "Epoch   6  2250/ 6203 batches: lr 10.00, loss  3.62, perplexity:    37.30\n",
      "Epoch   6  2500/ 6203 batches: lr 10.00, loss  3.56, perplexity:    35.12\n",
      "Epoch   6  2750/ 6203 batches: lr 10.00, loss  3.59, perplexity:    36.40\n",
      "Epoch   6  3000/ 6203 batches: lr 10.00, loss  3.65, perplexity:    38.60\n",
      "Epoch   6  3250/ 6203 batches: lr 10.00, loss  3.56, perplexity:    35.24\n",
      "Epoch   6  3500/ 6203 batches: lr 10.00, loss  3.55, perplexity:    34.75\n",
      "Epoch   6  3750/ 6203 batches: lr 10.00, loss  3.61, perplexity:    37.10\n",
      "Epoch   6  4000/ 6203 batches: lr 10.00, loss  3.59, perplexity:    36.15\n",
      "Epoch   6  4250/ 6203 batches: lr 10.00, loss  3.55, perplexity:    34.77\n",
      "Epoch   6  4500/ 6203 batches: lr 10.00, loss  3.54, perplexity:    34.49\n",
      "Epoch   6  4750/ 6203 batches: lr 10.00, loss  3.57, perplexity:    35.65\n",
      "Epoch   6  5000/ 6203 batches: lr 10.00, loss  3.54, perplexity:    34.35\n",
      "Epoch   6  5250/ 6203 batches: lr 10.00, loss  3.56, perplexity:    35.29\n",
      "Epoch   6  5500/ 6203 batches: lr 10.00, loss  3.57, perplexity:    35.58\n",
      "Epoch   6  5750/ 6203 batches: lr 10.00, loss  3.60, perplexity:    36.53\n",
      "Epoch   6  6000/ 6203 batches: lr 10.00, loss  3.58, perplexity:    35.88\n",
      "============================================================================\n",
      "Epoch   6 results: time: 454.14s, validation loss  3.69, perplexity    40.09\n",
      "============================================================================\n",
      "Epoch   7   250/ 6203 batches: lr 10.00, loss  3.56, perplexity:    35.18\n",
      "Epoch   7   500/ 6203 batches: lr 10.00, loss  3.55, perplexity:    34.71\n",
      "Epoch   7   750/ 6203 batches: lr 10.00, loss  3.55, perplexity:    34.78\n",
      "Epoch   7  1000/ 6203 batches: lr 10.00, loss  3.48, perplexity:    32.43\n",
      "Epoch   7  1250/ 6203 batches: lr 10.00, loss  3.52, perplexity:    33.67\n",
      "Epoch   7  1500/ 6203 batches: lr 10.00, loss  3.53, perplexity:    34.02\n",
      "Epoch   7  1750/ 6203 batches: lr 10.00, loss  3.58, perplexity:    35.87\n",
      "Epoch   7  2000/ 6203 batches: lr 10.00, loss  3.59, perplexity:    36.06\n",
      "Epoch   7  2250/ 6203 batches: lr 10.00, loss  3.58, perplexity:    36.01\n",
      "Epoch   7  2500/ 6203 batches: lr 10.00, loss  3.52, perplexity:    33.72\n",
      "Epoch   7  2750/ 6203 batches: lr 10.00, loss  3.56, perplexity:    35.26\n",
      "Epoch   7  3000/ 6203 batches: lr 10.00, loss  3.62, perplexity:    37.30\n",
      "Epoch   7  3250/ 6203 batches: lr 10.00, loss  3.53, perplexity:    34.16\n",
      "Epoch   7  3500/ 6203 batches: lr 10.00, loss  3.51, perplexity:    33.53\n",
      "Epoch   7  3750/ 6203 batches: lr 10.00, loss  3.58, perplexity:    35.80\n",
      "Epoch   7  4000/ 6203 batches: lr 10.00, loss  3.55, perplexity:    34.96\n",
      "Epoch   7  4250/ 6203 batches: lr 10.00, loss  3.51, perplexity:    33.55\n",
      "Epoch   7  4500/ 6203 batches: lr 10.00, loss  3.51, perplexity:    33.41\n",
      "Epoch   7  4750/ 6203 batches: lr 10.00, loss  3.53, perplexity:    34.27\n",
      "Epoch   7  5000/ 6203 batches: lr 10.00, loss  3.50, perplexity:    33.20\n",
      "Epoch   7  5250/ 6203 batches: lr 10.00, loss  3.53, perplexity:    34.00\n",
      "Epoch   7  5500/ 6203 batches: lr 10.00, loss  3.54, perplexity:    34.42\n",
      "Epoch   7  5750/ 6203 batches: lr 10.00, loss  3.57, perplexity:    35.35\n",
      "Epoch   7  6000/ 6203 batches: lr 10.00, loss  3.55, perplexity:    34.94\n",
      "============================================================================\n",
      "Epoch   7 results: time: 453.89s, validation loss  3.68, perplexity    39.50\n",
      "============================================================================\n",
      "Epoch   8   250/ 6203 batches: lr 10.00, loss  3.53, perplexity:    34.00\n",
      "Epoch   8   500/ 6203 batches: lr 10.00, loss  3.52, perplexity:    33.70\n",
      "Epoch   8   750/ 6203 batches: lr 10.00, loss  3.51, perplexity:    33.61\n",
      "Epoch   8  1000/ 6203 batches: lr 10.00, loss  3.45, perplexity:    31.53\n",
      "Epoch   8  1250/ 6203 batches: lr 10.00, loss  3.49, perplexity:    32.77\n",
      "Epoch   8  1500/ 6203 batches: lr 10.00, loss  3.50, perplexity:    33.11\n",
      "Epoch   8  1750/ 6203 batches: lr 10.00, loss  3.54, perplexity:    34.60\n",
      "Epoch   8  2000/ 6203 batches: lr 10.00, loss  3.56, perplexity:    35.07\n",
      "Epoch   8  2250/ 6203 batches: lr 10.00, loss  3.55, perplexity:    34.75\n",
      "Epoch   8  2500/ 6203 batches: lr 10.00, loss  3.49, perplexity:    32.80\n",
      "Epoch   8  2750/ 6203 batches: lr 10.00, loss  3.53, perplexity:    34.13\n",
      "Epoch   8  3000/ 6203 batches: lr 10.00, loss  3.59, perplexity:    36.20\n",
      "Epoch   8  3250/ 6203 batches: lr 10.00, loss  3.51, perplexity:    33.34\n",
      "Epoch   8  3500/ 6203 batches: lr 10.00, loss  3.49, perplexity:    32.78\n",
      "Epoch   8  3750/ 6203 batches: lr 10.00, loss  3.55, perplexity:    34.67\n",
      "Epoch   8  4000/ 6203 batches: lr 10.00, loss  3.52, perplexity:    33.82\n",
      "Epoch   8  4250/ 6203 batches: lr 10.00, loss  3.48, perplexity:    32.44\n",
      "Epoch   8  4500/ 6203 batches: lr 10.00, loss  3.48, perplexity:    32.46\n",
      "Epoch   8  4750/ 6203 batches: lr 10.00, loss  3.51, perplexity:    33.32\n",
      "Epoch   8  5000/ 6203 batches: lr 10.00, loss  3.48, perplexity:    32.39\n",
      "Epoch   8  5250/ 6203 batches: lr 10.00, loss  3.50, perplexity:    33.08\n",
      "Epoch   8  5500/ 6203 batches: lr 10.00, loss  3.51, perplexity:    33.50\n",
      "Epoch   8  5750/ 6203 batches: lr 10.00, loss  3.53, perplexity:    34.22\n",
      "Epoch   8  6000/ 6203 batches: lr 10.00, loss  3.52, perplexity:    33.90\n",
      "============================================================================\n",
      "Epoch   8 results: time: 453.83s, validation loss  3.66, perplexity    39.02\n",
      "============================================================================\n",
      "Epoch   9   250/ 6203 batches: lr 10.00, loss  3.50, perplexity:    33.05\n",
      "Epoch   9   500/ 6203 batches: lr 10.00, loss  3.49, perplexity:    32.75\n",
      "Epoch   9   750/ 6203 batches: lr 10.00, loss  3.49, perplexity:    32.74\n",
      "Epoch   9  1000/ 6203 batches: lr 10.00, loss  3.43, perplexity:    30.79\n",
      "Epoch   9  1250/ 6203 batches: lr 10.00, loss  3.46, perplexity:    31.87\n",
      "Epoch   9  1500/ 6203 batches: lr 10.00, loss  3.47, perplexity:    32.08\n",
      "Epoch   9  1750/ 6203 batches: lr 10.00, loss  3.52, perplexity:    33.79\n",
      "Epoch   9  2000/ 6203 batches: lr 10.00, loss  3.53, perplexity:    34.03\n",
      "Epoch   9  2250/ 6203 batches: lr 10.00, loss  3.52, perplexity:    33.89\n",
      "Epoch   9  2500/ 6203 batches: lr 10.00, loss  3.46, perplexity:    31.75\n",
      "Epoch   9  2750/ 6203 batches: lr 10.00, loss  3.50, perplexity:    33.10\n",
      "Epoch   9  3000/ 6203 batches: lr 10.00, loss  3.56, perplexity:    35.20\n",
      "Epoch   9  3250/ 6203 batches: lr 10.00, loss  3.47, perplexity:    32.27\n",
      "Epoch   9  3500/ 6203 batches: lr 10.00, loss  3.46, perplexity:    31.82\n",
      "Epoch   9  3750/ 6203 batches: lr 10.00, loss  3.53, perplexity:    34.03\n",
      "Epoch   9  4000/ 6203 batches: lr 10.00, loss  3.50, perplexity:    32.96\n",
      "Epoch   9  4250/ 6203 batches: lr 10.00, loss  3.46, perplexity:    31.82\n",
      "Epoch   9  4500/ 6203 batches: lr 10.00, loss  3.45, perplexity:    31.58\n",
      "Epoch   9  4750/ 6203 batches: lr 10.00, loss  3.48, perplexity:    32.32\n",
      "Epoch   9  5000/ 6203 batches: lr 10.00, loss  3.45, perplexity:    31.55\n",
      "Epoch   9  5250/ 6203 batches: lr 10.00, loss  3.47, perplexity:    32.15\n",
      "Epoch   9  5500/ 6203 batches: lr 10.00, loss  3.49, perplexity:    32.77\n",
      "Epoch   9  5750/ 6203 batches: lr 10.00, loss  3.51, perplexity:    33.35\n",
      "Epoch   9  6000/ 6203 batches: lr 10.00, loss  3.50, perplexity:    33.27\n",
      "============================================================================\n",
      "Epoch   9 results: time: 453.72s, validation loss  3.66, perplexity    38.77\n",
      "============================================================================\n",
      "Epoch  10   250/ 6203 batches: lr 10.00, loss  3.47, perplexity:    32.28\n",
      "Epoch  10   500/ 6203 batches: lr 10.00, loss  3.47, perplexity:    32.08\n",
      "Epoch  10   750/ 6203 batches: lr 10.00, loss  3.46, perplexity:    31.90\n",
      "Epoch  10  1000/ 6203 batches: lr 10.00, loss  3.40, perplexity:    30.09\n",
      "Epoch  10  1250/ 6203 batches: lr 10.00, loss  3.44, perplexity:    31.19\n",
      "Epoch  10  1500/ 6203 batches: lr 10.00, loss  3.44, perplexity:    31.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10  1750/ 6203 batches: lr 10.00, loss  3.50, perplexity:    33.04\n",
      "Epoch  10  2000/ 6203 batches: lr 10.00, loss  3.51, perplexity:    33.53\n",
      "Epoch  10  2250/ 6203 batches: lr 10.00, loss  3.50, perplexity:    33.14\n",
      "Epoch  10  2500/ 6203 batches: lr 10.00, loss  3.43, perplexity:    31.03\n",
      "Epoch  10  2750/ 6203 batches: lr 10.00, loss  3.48, perplexity:    32.57\n",
      "Epoch  10  3000/ 6203 batches: lr 10.00, loss  3.54, perplexity:    34.38\n",
      "Epoch  10  3250/ 6203 batches: lr 10.00, loss  3.45, perplexity:    31.59\n",
      "Epoch  10  3500/ 6203 batches: lr 10.00, loss  3.44, perplexity:    31.10\n",
      "Epoch  10  3750/ 6203 batches: lr 10.00, loss  3.50, perplexity:    33.11\n",
      "Epoch  10  4000/ 6203 batches: lr 10.00, loss  3.48, perplexity:    32.33\n",
      "Epoch  10  4250/ 6203 batches: lr 10.00, loss  3.44, perplexity:    31.11\n",
      "Epoch  10  4500/ 6203 batches: lr 10.00, loss  3.43, perplexity:    30.98\n",
      "Epoch  10  4750/ 6203 batches: lr 10.00, loss  3.46, perplexity:    31.90\n",
      "Epoch  10  5000/ 6203 batches: lr 10.00, loss  3.43, perplexity:    30.86\n",
      "Epoch  10  5250/ 6203 batches: lr 10.00, loss  3.45, perplexity:    31.61\n",
      "Epoch  10  5500/ 6203 batches: lr 10.00, loss  3.46, perplexity:    31.86\n",
      "Epoch  10  5750/ 6203 batches: lr 10.00, loss  3.49, perplexity:    32.65\n",
      "Epoch  10  6000/ 6203 batches: lr 10.00, loss  3.48, perplexity:    32.41\n",
      "============================================================================\n",
      "Epoch  10 results: time: 453.86s, validation loss  3.65, perplexity    38.43\n",
      "============================================================================\n",
      "Epoch  11   250/ 6203 batches: lr 10.00, loss  3.45, perplexity:    31.61\n",
      "Epoch  11   500/ 6203 batches: lr 10.00, loss  3.45, perplexity:    31.42\n",
      "Epoch  11   750/ 6203 batches: lr 10.00, loss  3.45, perplexity:    31.36\n",
      "Epoch  11  1000/ 6203 batches: lr 10.00, loss  3.38, perplexity:    29.47\n",
      "Epoch  11  1250/ 6203 batches: lr 10.00, loss  3.42, perplexity:    30.57\n",
      "Epoch  11  1500/ 6203 batches: lr 10.00, loss  3.43, perplexity:    30.73\n",
      "Epoch  11  1750/ 6203 batches: lr 10.00, loss  3.48, perplexity:    32.50\n",
      "Epoch  11  2000/ 6203 batches: lr 10.00, loss  3.49, perplexity:    32.66\n",
      "Epoch  11  2250/ 6203 batches: lr 10.00, loss  3.48, perplexity:    32.59\n",
      "Epoch  11  2500/ 6203 batches: lr 10.00, loss  3.42, perplexity:    30.55\n",
      "Epoch  11  2750/ 6203 batches: lr 10.00, loss  3.46, perplexity:    31.79\n",
      "Epoch  11  3000/ 6203 batches: lr 10.00, loss  3.52, perplexity:    33.80\n",
      "Epoch  11  3250/ 6203 batches: lr 10.00, loss  3.43, perplexity:    30.93\n",
      "Epoch  11  3500/ 6203 batches: lr 10.00, loss  3.42, perplexity:    30.50\n",
      "Epoch  11  3750/ 6203 batches: lr 10.00, loss  3.48, perplexity:    32.40\n",
      "Epoch  11  4000/ 6203 batches: lr 10.00, loss  3.46, perplexity:    31.77\n",
      "Epoch  11  4250/ 6203 batches: lr 10.00, loss  3.42, perplexity:    30.45\n",
      "Epoch  11  4500/ 6203 batches: lr 10.00, loss  3.41, perplexity:    30.37\n",
      "Epoch  11  4750/ 6203 batches: lr 10.00, loss  3.44, perplexity:    31.23\n",
      "Epoch  11  5000/ 6203 batches: lr 10.00, loss  3.41, perplexity:    30.27\n",
      "Epoch  11  5250/ 6203 batches: lr 10.00, loss  3.43, perplexity:    30.93\n",
      "Epoch  11  5500/ 6203 batches: lr 10.00, loss  3.44, perplexity:    31.26\n",
      "Epoch  11  5750/ 6203 batches: lr 10.00, loss  3.47, perplexity:    32.10\n",
      "Epoch  11  6000/ 6203 batches: lr 10.00, loss  3.46, perplexity:    31.94\n",
      "============================================================================\n",
      "Epoch  11 results: time: 453.86s, validation loss  3.65, perplexity    38.32\n",
      "============================================================================\n",
      "Epoch  12   250/ 6203 batches: lr 10.00, loss  3.44, perplexity:    31.11\n",
      "Epoch  12   500/ 6203 batches: lr 10.00, loss  3.43, perplexity:    30.77\n",
      "Epoch  12   750/ 6203 batches: lr 10.00, loss  3.42, perplexity:    30.64\n",
      "Epoch  12  1000/ 6203 batches: lr 10.00, loss  3.36, perplexity:    28.79\n",
      "Epoch  12  1250/ 6203 batches: lr 10.00, loss  3.40, perplexity:    30.04\n",
      "Epoch  12  1500/ 6203 batches: lr 10.00, loss  3.41, perplexity:    30.25\n",
      "Epoch  12  1750/ 6203 batches: lr 10.00, loss  3.46, perplexity:    31.88\n",
      "Epoch  12  2000/ 6203 batches: lr 10.00, loss  3.47, perplexity:    32.07\n",
      "Epoch  12  2250/ 6203 batches: lr 10.00, loss  3.47, perplexity:    32.13\n",
      "Epoch  12  2500/ 6203 batches: lr 10.00, loss  3.40, perplexity:    29.91\n",
      "Epoch  12  2750/ 6203 batches: lr 10.00, loss  3.44, perplexity:    31.15\n",
      "Epoch  12  3000/ 6203 batches: lr 10.00, loss  3.51, perplexity:    33.41\n",
      "Epoch  12  3250/ 6203 batches: lr 10.00, loss  3.41, perplexity:    30.37\n",
      "Epoch  12  3500/ 6203 batches: lr 10.00, loss  3.40, perplexity:    30.10\n",
      "Epoch  12  3750/ 6203 batches: lr 10.00, loss  3.46, perplexity:    31.84\n",
      "Epoch  12  4000/ 6203 batches: lr 10.00, loss  3.44, perplexity:    31.13\n",
      "Epoch  12  4250/ 6203 batches: lr 10.00, loss  3.40, perplexity:    30.01\n",
      "Epoch  12  4500/ 6203 batches: lr 10.00, loss  3.40, perplexity:    29.86\n",
      "Epoch  12  4750/ 6203 batches: lr 10.00, loss  3.42, perplexity:    30.69\n",
      "Epoch  12  5000/ 6203 batches: lr 10.00, loss  3.39, perplexity:    29.74\n",
      "Epoch  12  5250/ 6203 batches: lr 10.00, loss  3.41, perplexity:    30.29\n",
      "Epoch  12  5500/ 6203 batches: lr 10.00, loss  3.42, perplexity:    30.72\n",
      "Epoch  12  5750/ 6203 batches: lr 10.00, loss  3.45, perplexity:    31.62\n",
      "Epoch  12  6000/ 6203 batches: lr 10.00, loss  3.45, perplexity:    31.35\n",
      "============================================================================\n",
      "Epoch  12 results: time: 453.87s, validation loss  3.64, perplexity    38.14\n",
      "============================================================================\n",
      "Epoch  13   250/ 6203 batches: lr 10.00, loss  3.42, perplexity:    30.52\n",
      "Epoch  13   500/ 6203 batches: lr 10.00, loss  3.41, perplexity:    30.19\n",
      "Epoch  13   750/ 6203 batches: lr 10.00, loss  3.41, perplexity:    30.24\n",
      "Epoch  13  1000/ 6203 batches: lr 10.00, loss  3.35, perplexity:    28.43\n",
      "Epoch  13  1250/ 6203 batches: lr 10.00, loss  3.38, perplexity:    29.37\n",
      "Epoch  13  1500/ 6203 batches: lr 10.00, loss  3.39, perplexity:    29.70\n",
      "Epoch  13  1750/ 6203 batches: lr 10.00, loss  3.44, perplexity:    31.22\n",
      "Epoch  13  2000/ 6203 batches: lr 10.00, loss  3.45, perplexity:    31.45\n",
      "Epoch  13  2250/ 6203 batches: lr 10.00, loss  3.45, perplexity:    31.41\n",
      "Epoch  13  2500/ 6203 batches: lr 10.00, loss  3.38, perplexity:    29.51\n",
      "Epoch  13  2750/ 6203 batches: lr 10.00, loss  3.43, perplexity:    30.85\n",
      "Epoch  13  3000/ 6203 batches: lr 10.00, loss  3.49, perplexity:    32.70\n",
      "Epoch  13  3250/ 6203 batches: lr 10.00, loss  3.40, perplexity:    29.92\n",
      "Epoch  13  3500/ 6203 batches: lr 10.00, loss  3.38, perplexity:    29.49\n",
      "Epoch  13  3750/ 6203 batches: lr 10.00, loss  3.44, perplexity:    31.26\n",
      "Epoch  13  4000/ 6203 batches: lr 10.00, loss  3.42, perplexity:    30.69\n",
      "Epoch  13  4250/ 6203 batches: lr 10.00, loss  3.39, perplexity:    29.66\n",
      "Epoch  13  4500/ 6203 batches: lr 10.00, loss  3.39, perplexity:    29.55\n",
      "Epoch  13  4750/ 6203 batches: lr 10.00, loss  3.41, perplexity:    30.16\n",
      "Epoch  13  5000/ 6203 batches: lr 10.00, loss  3.38, perplexity:    29.33\n",
      "Epoch  13  5250/ 6203 batches: lr 10.00, loss  3.40, perplexity:    29.88\n",
      "Epoch  13  5500/ 6203 batches: lr 10.00, loss  3.41, perplexity:    30.20\n",
      "Epoch  13  5750/ 6203 batches: lr 10.00, loss  3.43, perplexity:    30.98\n",
      "Epoch  13  6000/ 6203 batches: lr 10.00, loss  3.43, perplexity:    30.74\n",
      "============================================================================\n",
      "Epoch  13 results: time: 453.86s, validation loss  3.64, perplexity    37.92\n",
      "============================================================================\n",
      "Epoch  14   250/ 6203 batches: lr 10.00, loss  3.40, perplexity:    30.02\n",
      "Epoch  14   500/ 6203 batches: lr 10.00, loss  3.39, perplexity:    29.77\n",
      "Epoch  14   750/ 6203 batches: lr 10.00, loss  3.40, perplexity:    29.92\n",
      "Epoch  14  1000/ 6203 batches: lr 10.00, loss  3.33, perplexity:    27.96\n",
      "Epoch  14  1250/ 6203 batches: lr 10.00, loss  3.37, perplexity:    29.13\n",
      "Epoch  14  1500/ 6203 batches: lr 10.00, loss  3.38, perplexity:    29.30\n",
      "Epoch  14  1750/ 6203 batches: lr 10.00, loss  3.43, perplexity:    30.77\n",
      "Epoch  14  2000/ 6203 batches: lr 10.00, loss  3.43, perplexity:    31.00\n",
      "Epoch  14  2250/ 6203 batches: lr 10.00, loss  3.43, perplexity:    30.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  14  2500/ 6203 batches: lr 10.00, loss  3.37, perplexity:    29.01\n",
      "Epoch  14  2750/ 6203 batches: lr 10.00, loss  3.41, perplexity:    30.27\n",
      "Epoch  14  3000/ 6203 batches: lr 10.00, loss  3.47, perplexity:    32.16\n",
      "Epoch  14  3250/ 6203 batches: lr 10.00, loss  3.38, perplexity:    29.27\n",
      "Epoch  14  3500/ 6203 batches: lr 10.00, loss  3.37, perplexity:    29.03\n",
      "Epoch  14  3750/ 6203 batches: lr 10.00, loss  3.43, perplexity:    30.88\n",
      "Epoch  14  4000/ 6203 batches: lr 10.00, loss  3.40, perplexity:    30.05\n",
      "Epoch  14  4250/ 6203 batches: lr 10.00, loss  3.37, perplexity:    29.05\n",
      "Epoch  14  4500/ 6203 batches: lr 10.00, loss  3.36, perplexity:    28.85\n",
      "Epoch  14  4750/ 6203 batches: lr 10.00, loss  3.39, perplexity:    29.66\n",
      "Epoch  14  5000/ 6203 batches: lr 10.00, loss  3.37, perplexity:    28.98\n",
      "Epoch  14  5250/ 6203 batches: lr 10.00, loss  3.38, perplexity:    29.48\n",
      "Epoch  14  5500/ 6203 batches: lr 10.00, loss  3.40, perplexity:    29.84\n",
      "Epoch  14  5750/ 6203 batches: lr 10.00, loss  3.42, perplexity:    30.53\n",
      "Epoch  14  6000/ 6203 batches: lr 10.00, loss  3.42, perplexity:    30.44\n",
      "============================================================================\n",
      "Epoch  14 results: time: 456.45s, validation loss  3.64, perplexity    37.96\n",
      "============================================================================\n",
      "Epoch  15   250/ 6203 batches: lr 2.50, loss  3.39, perplexity:    29.60\n",
      "Epoch  15   500/ 6203 batches: lr 2.50, loss  3.37, perplexity:    29.13\n",
      "Epoch  15   750/ 6203 batches: lr 2.50, loss  3.37, perplexity:    28.96\n",
      "Epoch  15  1000/ 6203 batches: lr 2.50, loss  3.30, perplexity:    26.99\n",
      "Epoch  15  1250/ 6203 batches: lr 2.50, loss  3.33, perplexity:    28.01\n",
      "Epoch  15  1500/ 6203 batches: lr 2.50, loss  3.32, perplexity:    27.74\n",
      "Epoch  15  1750/ 6203 batches: lr 2.50, loss  3.37, perplexity:    29.16\n",
      "Epoch  15  2000/ 6203 batches: lr 2.50, loss  3.38, perplexity:    29.51\n",
      "Epoch  15  2250/ 6203 batches: lr 2.50, loss  3.37, perplexity:    29.22\n",
      "Epoch  15  2500/ 6203 batches: lr 2.50, loss  3.31, perplexity:    27.28\n",
      "Epoch  15  2750/ 6203 batches: lr 2.50, loss  3.35, perplexity:    28.51\n",
      "Epoch  15  3000/ 6203 batches: lr 2.50, loss  3.40, perplexity:    29.96\n",
      "Epoch  15  3250/ 6203 batches: lr 2.50, loss  3.31, perplexity:    27.31\n",
      "Epoch  15  3500/ 6203 batches: lr 2.50, loss  3.30, perplexity:    27.03\n",
      "Epoch  15  3750/ 6203 batches: lr 2.50, loss  3.35, perplexity:    28.39\n",
      "Epoch  15  4000/ 6203 batches: lr 2.50, loss  3.33, perplexity:    27.82\n",
      "Epoch  15  4250/ 6203 batches: lr 2.50, loss  3.29, perplexity:    26.80\n",
      "Epoch  15  4500/ 6203 batches: lr 2.50, loss  3.29, perplexity:    26.71\n",
      "Epoch  15  4750/ 6203 batches: lr 2.50, loss  3.30, perplexity:    27.09\n",
      "Epoch  15  5000/ 6203 batches: lr 2.50, loss  3.27, perplexity:    26.25\n",
      "Epoch  15  5250/ 6203 batches: lr 2.50, loss  3.28, perplexity:    26.68\n",
      "Epoch  15  5500/ 6203 batches: lr 2.50, loss  3.29, perplexity:    26.90\n",
      "Epoch  15  5750/ 6203 batches: lr 2.50, loss  3.31, perplexity:    27.43\n",
      "Epoch  15  6000/ 6203 batches: lr 2.50, loss  3.30, perplexity:    27.18\n",
      "============================================================================\n",
      "Epoch  15 results: time: 453.44s, validation loss  3.61, perplexity    37.13\n",
      "============================================================================\n",
      "Epoch  16   250/ 6203 batches: lr 2.50, loss  3.32, perplexity:    27.76\n",
      "Epoch  16   500/ 6203 batches: lr 2.50, loss  3.31, perplexity:    27.31\n",
      "Epoch  16   750/ 6203 batches: lr 2.50, loss  3.31, perplexity:    27.33\n",
      "Epoch  16  1000/ 6203 batches: lr 2.50, loss  3.25, perplexity:    25.70\n",
      "Epoch  16  1250/ 6203 batches: lr 2.50, loss  3.28, perplexity:    26.64\n",
      "Epoch  16  1500/ 6203 batches: lr 2.50, loss  3.29, perplexity:    26.76\n",
      "Epoch  16  1750/ 6203 batches: lr 2.50, loss  3.33, perplexity:    27.91\n",
      "Epoch  16  2000/ 6203 batches: lr 2.50, loss  3.34, perplexity:    28.25\n",
      "Epoch  16  2250/ 6203 batches: lr 2.50, loss  3.33, perplexity:    28.06\n",
      "Epoch  16  2500/ 6203 batches: lr 2.50, loss  3.27, perplexity:    26.24\n",
      "Epoch  16  2750/ 6203 batches: lr 2.50, loss  3.31, perplexity:    27.26\n",
      "Epoch  16  3000/ 6203 batches: lr 2.50, loss  3.37, perplexity:    29.06\n",
      "Epoch  16  3250/ 6203 batches: lr 2.50, loss  3.28, perplexity:    26.65\n",
      "Epoch  16  3500/ 6203 batches: lr 2.50, loss  3.26, perplexity:    26.17\n",
      "Epoch  16  3750/ 6203 batches: lr 2.50, loss  3.32, perplexity:    27.70\n",
      "Epoch  16  4000/ 6203 batches: lr 2.50, loss  3.29, perplexity:    26.94\n",
      "Epoch  16  4250/ 6203 batches: lr 2.50, loss  3.26, perplexity:    25.99\n",
      "Epoch  16  4500/ 6203 batches: lr 2.50, loss  3.26, perplexity:    25.97\n",
      "Epoch  16  4750/ 6203 batches: lr 2.50, loss  3.27, perplexity:    26.42\n",
      "Epoch  16  5000/ 6203 batches: lr 2.50, loss  3.24, perplexity:    25.63\n",
      "Epoch  16  5250/ 6203 batches: lr 2.50, loss  3.27, perplexity:    26.19\n",
      "Epoch  16  5500/ 6203 batches: lr 2.50, loss  3.27, perplexity:    26.33\n",
      "Epoch  16  5750/ 6203 batches: lr 2.50, loss  3.29, perplexity:    26.86\n",
      "Epoch  16  6000/ 6203 batches: lr 2.50, loss  3.29, perplexity:    26.79\n",
      "============================================================================\n",
      "Epoch  16 results: time: 453.91s, validation loss  3.61, perplexity    36.92\n",
      "============================================================================\n",
      "Epoch  17   250/ 6203 batches: lr 2.50, loss  3.29, perplexity:    26.97\n",
      "Epoch  17   500/ 6203 batches: lr 2.50, loss  3.28, perplexity:    26.68\n",
      "Epoch  17   750/ 6203 batches: lr 2.50, loss  3.28, perplexity:    26.70\n",
      "Epoch  17  1000/ 6203 batches: lr 2.50, loss  3.21, perplexity:    24.89\n",
      "Epoch  17  1250/ 6203 batches: lr 2.50, loss  3.26, perplexity:    26.13\n",
      "Epoch  17  1500/ 6203 batches: lr 2.50, loss  3.26, perplexity:    26.14\n",
      "Epoch  17  1750/ 6203 batches: lr 2.50, loss  3.31, perplexity:    27.46\n",
      "Epoch  17  2000/ 6203 batches: lr 2.50, loss  3.32, perplexity:    27.62\n",
      "Epoch  17  2250/ 6203 batches: lr 2.50, loss  3.31, perplexity:    27.42\n",
      "Epoch  17  2500/ 6203 batches: lr 2.50, loss  3.24, perplexity:    25.62\n",
      "Epoch  17  2750/ 6203 batches: lr 2.50, loss  3.29, perplexity:    26.73\n",
      "Epoch  17  3000/ 6203 batches: lr 2.50, loss  3.35, perplexity:    28.53\n",
      "Epoch  17  3250/ 6203 batches: lr 2.50, loss  3.26, perplexity:    26.01\n",
      "Epoch  17  3500/ 6203 batches: lr 2.50, loss  3.24, perplexity:    25.59\n",
      "Epoch  17  3750/ 6203 batches: lr 2.50, loss  3.30, perplexity:    27.03\n",
      "Epoch  17  4000/ 6203 batches: lr 2.50, loss  3.28, perplexity:    26.47\n",
      "Epoch  17  4250/ 6203 batches: lr 2.50, loss  3.24, perplexity:    25.49\n",
      "Epoch  17  4500/ 6203 batches: lr 2.50, loss  3.24, perplexity:    25.53\n",
      "Epoch  17  4750/ 6203 batches: lr 2.50, loss  3.26, perplexity:    26.06\n",
      "Epoch  17  5000/ 6203 batches: lr 2.50, loss  3.23, perplexity:    25.24\n",
      "Epoch  17  5250/ 6203 batches: lr 2.50, loss  3.25, perplexity:    25.70\n",
      "Epoch  17  5500/ 6203 batches: lr 2.50, loss  3.25, perplexity:    25.86\n",
      "Epoch  17  5750/ 6203 batches: lr 2.50, loss  3.28, perplexity:    26.47\n",
      "Epoch  17  6000/ 6203 batches: lr 2.50, loss  3.27, perplexity:    26.35\n",
      "============================================================================\n",
      "Epoch  17 results: time: 454.11s, validation loss  3.61, perplexity    36.99\n",
      "============================================================================\n",
      "Epoch  18   250/ 6203 batches: lr 0.62, loss  3.28, perplexity:    26.67\n",
      "Epoch  18   500/ 6203 batches: lr 0.62, loss  3.28, perplexity:    26.65\n",
      "Epoch  18   750/ 6203 batches: lr 0.62, loss  3.28, perplexity:    26.49\n",
      "Epoch  18  1000/ 6203 batches: lr 0.62, loss  3.21, perplexity:    24.82\n",
      "Epoch  18  1250/ 6203 batches: lr 0.62, loss  3.25, perplexity:    25.72\n",
      "Epoch  18  1500/ 6203 batches: lr 0.62, loss  3.25, perplexity:    25.79\n",
      "Epoch  18  1750/ 6203 batches: lr 0.62, loss  3.29, perplexity:    26.97\n",
      "Epoch  18  2000/ 6203 batches: lr 0.62, loss  3.31, perplexity:    27.33\n",
      "Epoch  18  2250/ 6203 batches: lr 0.62, loss  3.30, perplexity:    27.15\n",
      "Epoch  18  2500/ 6203 batches: lr 0.62, loss  3.23, perplexity:    25.29\n",
      "Epoch  18  2750/ 6203 batches: lr 0.62, loss  3.27, perplexity:    26.34\n",
      "Epoch  18  3000/ 6203 batches: lr 0.62, loss  3.33, perplexity:    28.04\n",
      "Epoch  18  3250/ 6203 batches: lr 0.62, loss  3.24, perplexity:    25.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  18  3500/ 6203 batches: lr 0.62, loss  3.22, perplexity:    25.14\n",
      "Epoch  18  3750/ 6203 batches: lr 0.62, loss  3.28, perplexity:    26.57\n",
      "Epoch  18  4000/ 6203 batches: lr 0.62, loss  3.26, perplexity:    25.97\n",
      "Epoch  18  4250/ 6203 batches: lr 0.62, loss  3.22, perplexity:    25.08\n",
      "Epoch  18  4500/ 6203 batches: lr 0.62, loss  3.22, perplexity:    24.94\n",
      "Epoch  18  4750/ 6203 batches: lr 0.62, loss  3.24, perplexity:    25.49\n",
      "Epoch  18  5000/ 6203 batches: lr 0.62, loss  3.21, perplexity:    24.79\n",
      "Epoch  18  5250/ 6203 batches: lr 0.62, loss  3.22, perplexity:    25.13\n",
      "Epoch  18  5500/ 6203 batches: lr 0.62, loss  3.23, perplexity:    25.19\n",
      "Epoch  18  5750/ 6203 batches: lr 0.62, loss  3.25, perplexity:    25.76\n",
      "Epoch  18  6000/ 6203 batches: lr 0.62, loss  3.25, perplexity:    25.72\n",
      "============================================================================\n",
      "Epoch  18 results: time: 454.12s, validation loss  3.60, perplexity    36.68\n",
      "============================================================================\n",
      "Epoch  19   250/ 6203 batches: lr 0.62, loss  3.27, perplexity:    26.24\n",
      "Epoch  19   500/ 6203 batches: lr 0.62, loss  3.27, perplexity:    26.25\n",
      "Epoch  19   750/ 6203 batches: lr 0.62, loss  3.26, perplexity:    26.07\n",
      "Epoch  19  1000/ 6203 batches: lr 0.62, loss  3.20, perplexity:    24.48\n",
      "Epoch  19  1250/ 6203 batches: lr 0.62, loss  3.23, perplexity:    25.40\n",
      "Epoch  19  1500/ 6203 batches: lr 0.62, loss  3.23, perplexity:    25.40\n",
      "Epoch  19  1750/ 6203 batches: lr 0.62, loss  3.28, perplexity:    26.63\n",
      "Epoch  19  2000/ 6203 batches: lr 0.62, loss  3.29, perplexity:    26.97\n",
      "Epoch  19  2250/ 6203 batches: lr 0.62, loss  3.29, perplexity:    26.88\n",
      "Epoch  19  2500/ 6203 batches: lr 0.62, loss  3.22, perplexity:    24.94\n",
      "Epoch  19  2750/ 6203 batches: lr 0.62, loss  3.26, perplexity:    26.01\n",
      "Epoch  19  3000/ 6203 batches: lr 0.62, loss  3.32, perplexity:    27.70\n",
      "Epoch  19  3250/ 6203 batches: lr 0.62, loss  3.23, perplexity:    25.28\n",
      "Epoch  19  3500/ 6203 batches: lr 0.62, loss  3.22, perplexity:    24.93\n",
      "Epoch  19  3750/ 6203 batches: lr 0.62, loss  3.27, perplexity:    26.21\n",
      "Epoch  19  4000/ 6203 batches: lr 0.62, loss  3.25, perplexity:    25.71\n",
      "Epoch  19  4250/ 6203 batches: lr 0.62, loss  3.21, perplexity:    24.75\n",
      "Epoch  19  4500/ 6203 batches: lr 0.62, loss  3.21, perplexity:    24.67\n",
      "Epoch  19  4750/ 6203 batches: lr 0.62, loss  3.23, perplexity:    25.28\n",
      "Epoch  19  5000/ 6203 batches: lr 0.62, loss  3.20, perplexity:    24.54\n",
      "Epoch  19  5250/ 6203 batches: lr 0.62, loss  3.21, perplexity:    24.87\n",
      "Epoch  19  5500/ 6203 batches: lr 0.62, loss  3.22, perplexity:    25.04\n",
      "Epoch  19  5750/ 6203 batches: lr 0.62, loss  3.25, perplexity:    25.72\n",
      "Epoch  19  6000/ 6203 batches: lr 0.62, loss  3.24, perplexity:    25.53\n",
      "============================================================================\n",
      "Epoch  19 results: time: 454.28s, validation loss  3.60, perplexity    36.62\n",
      "============================================================================\n",
      "Epoch  20   250/ 6203 batches: lr 0.62, loss  3.26, perplexity:    26.13\n",
      "Epoch  20   500/ 6203 batches: lr 0.62, loss  3.26, perplexity:    25.96\n",
      "Epoch  20   750/ 6203 batches: lr 0.62, loss  3.25, perplexity:    25.81\n",
      "Epoch  20  1000/ 6203 batches: lr 0.62, loss  3.19, perplexity:    24.29\n",
      "Epoch  20  1250/ 6203 batches: lr 0.62, loss  3.23, perplexity:    25.29\n",
      "Epoch  20  1500/ 6203 batches: lr 0.62, loss  3.22, perplexity:    25.15\n",
      "Epoch  20  1750/ 6203 batches: lr 0.62, loss  3.27, perplexity:    26.31\n",
      "Epoch  20  2000/ 6203 batches: lr 0.62, loss  3.29, perplexity:    26.76\n",
      "Epoch  20  2250/ 6203 batches: lr 0.62, loss  3.28, perplexity:    26.51\n",
      "Epoch  20  2500/ 6203 batches: lr 0.62, loss  3.21, perplexity:    24.75\n",
      "Epoch  20  2750/ 6203 batches: lr 0.62, loss  3.26, perplexity:    25.92\n",
      "Epoch  20  3000/ 6203 batches: lr 0.62, loss  3.31, perplexity:    27.44\n",
      "Epoch  20  3250/ 6203 batches: lr 0.62, loss  3.22, perplexity:    25.12\n",
      "Epoch  20  3500/ 6203 batches: lr 0.62, loss  3.21, perplexity:    24.85\n",
      "Epoch  20  3750/ 6203 batches: lr 0.62, loss  3.27, perplexity:    26.20\n",
      "Epoch  20  4000/ 6203 batches: lr 0.62, loss  3.24, perplexity:    25.58\n",
      "Epoch  20  4250/ 6203 batches: lr 0.62, loss  3.21, perplexity:    24.79\n",
      "Epoch  20  4500/ 6203 batches: lr 0.62, loss  3.20, perplexity:    24.57\n",
      "Epoch  20  4750/ 6203 batches: lr 0.62, loss  3.22, perplexity:    25.10\n",
      "Epoch  20  5000/ 6203 batches: lr 0.62, loss  3.20, perplexity:    24.43\n",
      "Epoch  20  5250/ 6203 batches: lr 0.62, loss  3.21, perplexity:    24.68\n",
      "Epoch  20  5500/ 6203 batches: lr 0.62, loss  3.21, perplexity:    24.87\n",
      "Epoch  20  5750/ 6203 batches: lr 0.62, loss  3.24, perplexity:    25.45\n",
      "Epoch  20  6000/ 6203 batches: lr 0.62, loss  3.24, perplexity:    25.48\n",
      "============================================================================\n",
      "Epoch  20 results: time: 454.02s, validation loss  3.60, perplexity    36.61\n",
      "============================================================================\n",
      "Epoch  21   250/ 6203 batches: lr 0.62, loss  3.26, perplexity:    25.94\n",
      "Epoch  21   500/ 6203 batches: lr 0.62, loss  3.25, perplexity:    25.68\n",
      "Epoch  21   750/ 6203 batches: lr 0.62, loss  3.24, perplexity:    25.63\n",
      "Epoch  21  1000/ 6203 batches: lr 0.62, loss  3.18, perplexity:    24.07\n",
      "Epoch  21  1250/ 6203 batches: lr 0.62, loss  3.22, perplexity:    24.99\n",
      "Epoch  21  1500/ 6203 batches: lr 0.62, loss  3.22, perplexity:    24.94\n",
      "Epoch  21  1750/ 6203 batches: lr 0.62, loss  3.26, perplexity:    26.15\n",
      "Epoch  21  2000/ 6203 batches: lr 0.62, loss  3.28, perplexity:    26.53\n",
      "Epoch  21  2250/ 6203 batches: lr 0.62, loss  3.28, perplexity:    26.54\n",
      "Epoch  21  2500/ 6203 batches: lr 0.62, loss  3.21, perplexity:    24.69\n",
      "Epoch  21  2750/ 6203 batches: lr 0.62, loss  3.25, perplexity:    25.75\n",
      "Epoch  21  3000/ 6203 batches: lr 0.62, loss  3.31, perplexity:    27.26\n",
      "Epoch  21  3250/ 6203 batches: lr 0.62, loss  3.21, perplexity:    24.88\n",
      "Epoch  21  3500/ 6203 batches: lr 0.62, loss  3.21, perplexity:    24.71\n",
      "Epoch  21  3750/ 6203 batches: lr 0.62, loss  3.26, perplexity:    26.12\n",
      "Epoch  21  4000/ 6203 batches: lr 0.62, loss  3.23, perplexity:    25.29\n",
      "Epoch  21  4250/ 6203 batches: lr 0.62, loss  3.21, perplexity:    24.67\n",
      "Epoch  21  4500/ 6203 batches: lr 0.62, loss  3.20, perplexity:    24.43\n",
      "Epoch  21  4750/ 6203 batches: lr 0.62, loss  3.22, perplexity:    24.92\n",
      "Epoch  21  5000/ 6203 batches: lr 0.62, loss  3.19, perplexity:    24.25\n",
      "Epoch  21  5250/ 6203 batches: lr 0.62, loss  3.21, perplexity:    24.72\n",
      "Epoch  21  5500/ 6203 batches: lr 0.62, loss  3.21, perplexity:    24.78\n",
      "Epoch  21  5750/ 6203 batches: lr 0.62, loss  3.24, perplexity:    25.51\n",
      "Epoch  21  6000/ 6203 batches: lr 0.62, loss  3.23, perplexity:    25.17\n",
      "============================================================================\n",
      "Epoch  21 results: time: 454.35s, validation loss  3.60, perplexity    36.61\n",
      "============================================================================\n",
      "Epoch  22   250/ 6203 batches: lr 0.16, loss  3.25, perplexity:    25.76\n",
      "Epoch  22   500/ 6203 batches: lr 0.16, loss  3.25, perplexity:    25.84\n",
      "Epoch  22   750/ 6203 batches: lr 0.16, loss  3.24, perplexity:    25.66\n",
      "Epoch  22  1000/ 6203 batches: lr 0.16, loss  3.18, perplexity:    24.08\n",
      "Epoch  22  1250/ 6203 batches: lr 0.16, loss  3.22, perplexity:    25.13\n",
      "Epoch  22  1500/ 6203 batches: lr 0.16, loss  3.22, perplexity:    25.05\n",
      "Epoch  22  1750/ 6203 batches: lr 0.16, loss  3.27, perplexity:    26.21\n",
      "Epoch  22  2000/ 6203 batches: lr 0.16, loss  3.28, perplexity:    26.61\n",
      "Epoch  22  2250/ 6203 batches: lr 0.16, loss  3.28, perplexity:    26.46\n",
      "Epoch  22  2500/ 6203 batches: lr 0.16, loss  3.20, perplexity:    24.59\n",
      "Epoch  22  2750/ 6203 batches: lr 0.16, loss  3.24, perplexity:    25.65\n",
      "Epoch  22  3000/ 6203 batches: lr 0.16, loss  3.30, perplexity:    27.12\n",
      "Epoch  22  3250/ 6203 batches: lr 0.16, loss  3.22, perplexity:    24.91\n",
      "Epoch  22  3500/ 6203 batches: lr 0.16, loss  3.20, perplexity:    24.47\n",
      "Epoch  22  3750/ 6203 batches: lr 0.16, loss  3.26, perplexity:    25.98\n",
      "Epoch  22  4000/ 6203 batches: lr 0.16, loss  3.24, perplexity:    25.44\n",
      "Epoch  22  4250/ 6203 batches: lr 0.16, loss  3.20, perplexity:    24.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  22  4500/ 6203 batches: lr 0.16, loss  3.19, perplexity:    24.24\n",
      "Epoch  22  4750/ 6203 batches: lr 0.16, loss  3.21, perplexity:    24.74\n",
      "Epoch  22  5000/ 6203 batches: lr 0.16, loss  3.18, perplexity:    24.16\n",
      "Epoch  22  5250/ 6203 batches: lr 0.16, loss  3.20, perplexity:    24.45\n",
      "Epoch  22  5500/ 6203 batches: lr 0.16, loss  3.20, perplexity:    24.63\n",
      "Epoch  22  5750/ 6203 batches: lr 0.16, loss  3.22, perplexity:    25.10\n",
      "Epoch  22  6000/ 6203 batches: lr 0.16, loss  3.23, perplexity:    25.25\n",
      "============================================================================\n",
      "Epoch  22 results: time: 454.37s, validation loss  3.60, perplexity    36.50\n",
      "============================================================================\n",
      "Epoch  23   250/ 6203 batches: lr 0.16, loss  3.24, perplexity:    25.63\n",
      "Epoch  23   500/ 6203 batches: lr 0.16, loss  3.24, perplexity:    25.59\n",
      "Epoch  23   750/ 6203 batches: lr 0.16, loss  3.24, perplexity:    25.56\n",
      "Epoch  23  1000/ 6203 batches: lr 0.16, loss  3.17, perplexity:    23.90\n",
      "Epoch  23  1250/ 6203 batches: lr 0.16, loss  3.21, perplexity:    24.87\n",
      "Epoch  23  1500/ 6203 batches: lr 0.16, loss  3.21, perplexity:    24.83\n",
      "Epoch  23  1750/ 6203 batches: lr 0.16, loss  3.27, perplexity:    26.20\n",
      "Epoch  23  2000/ 6203 batches: lr 0.16, loss  3.27, perplexity:    26.42\n",
      "Epoch  23  2250/ 6203 batches: lr 0.16, loss  3.27, perplexity:    26.42\n",
      "Epoch  23  2500/ 6203 batches: lr 0.16, loss  3.20, perplexity:    24.45\n",
      "Epoch  23  2750/ 6203 batches: lr 0.16, loss  3.24, perplexity:    25.49\n",
      "Epoch  23  3000/ 6203 batches: lr 0.16, loss  3.30, perplexity:    27.06\n",
      "Epoch  23  3250/ 6203 batches: lr 0.16, loss  3.21, perplexity:    24.83\n",
      "Epoch  23  3500/ 6203 batches: lr 0.16, loss  3.19, perplexity:    24.28\n",
      "Epoch  23  3750/ 6203 batches: lr 0.16, loss  3.25, perplexity:    25.90\n",
      "Epoch  23  4000/ 6203 batches: lr 0.16, loss  3.23, perplexity:    25.27\n",
      "Epoch  23  4250/ 6203 batches: lr 0.16, loss  3.20, perplexity:    24.53\n",
      "Epoch  23  4500/ 6203 batches: lr 0.16, loss  3.18, perplexity:    24.08\n",
      "Epoch  23  4750/ 6203 batches: lr 0.16, loss  3.21, perplexity:    24.69\n",
      "Epoch  23  5000/ 6203 batches: lr 0.16, loss  3.19, perplexity:    24.17\n",
      "Epoch  23  5250/ 6203 batches: lr 0.16, loss  3.19, perplexity:    24.40\n",
      "Epoch  23  5500/ 6203 batches: lr 0.16, loss  3.20, perplexity:    24.55\n",
      "Epoch  23  5750/ 6203 batches: lr 0.16, loss  3.23, perplexity:    25.17\n",
      "Epoch  23  6000/ 6203 batches: lr 0.16, loss  3.22, perplexity:    25.05\n",
      "============================================================================\n",
      "Epoch  23 results: time: 454.29s, validation loss  3.60, perplexity    36.50\n",
      "============================================================================\n",
      "Epoch  24   250/ 6203 batches: lr 0.04, loss  3.25, perplexity:    25.73\n",
      "Epoch  24   500/ 6203 batches: lr 0.04, loss  3.25, perplexity:    25.69\n",
      "Epoch  24   750/ 6203 batches: lr 0.04, loss  3.24, perplexity:    25.62\n",
      "Epoch  24  1000/ 6203 batches: lr 0.04, loss  3.17, perplexity:    23.93\n",
      "Epoch  24  1250/ 6203 batches: lr 0.04, loss  3.22, perplexity:    24.92\n",
      "Epoch  24  1500/ 6203 batches: lr 0.04, loss  3.21, perplexity:    24.86\n",
      "Epoch  24  1750/ 6203 batches: lr 0.04, loss  3.26, perplexity:    26.10\n",
      "Epoch  24  2000/ 6203 batches: lr 0.04, loss  3.28, perplexity:    26.49\n",
      "Epoch  24  2250/ 6203 batches: lr 0.04, loss  3.27, perplexity:    26.35\n",
      "Epoch  24  2500/ 6203 batches: lr 0.04, loss  3.20, perplexity:    24.44\n",
      "Epoch  24  2750/ 6203 batches: lr 0.04, loss  3.24, perplexity:    25.55\n",
      "Epoch  24  3000/ 6203 batches: lr 0.04, loss  3.30, perplexity:    27.00\n",
      "Epoch  24  3250/ 6203 batches: lr 0.04, loss  3.21, perplexity:    24.72\n",
      "Epoch  24  3500/ 6203 batches: lr 0.04, loss  3.20, perplexity:    24.43\n",
      "Epoch  24  3750/ 6203 batches: lr 0.04, loss  3.26, perplexity:    25.95\n",
      "Epoch  24  4000/ 6203 batches: lr 0.04, loss  3.23, perplexity:    25.34\n",
      "Epoch  24  4250/ 6203 batches: lr 0.04, loss  3.19, perplexity:    24.32\n",
      "Epoch  24  4500/ 6203 batches: lr 0.04, loss  3.19, perplexity:    24.24\n",
      "Epoch  24  4750/ 6203 batches: lr 0.04, loss  3.21, perplexity:    24.73\n",
      "Epoch  24  5000/ 6203 batches: lr 0.04, loss  3.18, perplexity:    24.00\n",
      "Epoch  24  5250/ 6203 batches: lr 0.04, loss  3.19, perplexity:    24.37\n",
      "Epoch  24  5500/ 6203 batches: lr 0.04, loss  3.20, perplexity:    24.52\n",
      "Epoch  24  5750/ 6203 batches: lr 0.04, loss  3.22, perplexity:    25.06\n",
      "Epoch  24  6000/ 6203 batches: lr 0.04, loss  3.22, perplexity:    24.97\n",
      "============================================================================\n",
      "Epoch  24 results: time: 454.26s, validation loss  3.60, perplexity    36.46\n",
      "============================================================================\n",
      "Epoch  25   250/ 6203 batches: lr 0.04, loss  3.24, perplexity:    25.63\n",
      "Epoch  25   500/ 6203 batches: lr 0.04, loss  3.25, perplexity:    25.69\n",
      "Epoch  25   750/ 6203 batches: lr 0.04, loss  3.24, perplexity:    25.57\n",
      "Epoch  25  1000/ 6203 batches: lr 0.04, loss  3.17, perplexity:    23.90\n",
      "Epoch  25  1250/ 6203 batches: lr 0.04, loss  3.21, perplexity:    24.83\n",
      "Epoch  25  1500/ 6203 batches: lr 0.04, loss  3.21, perplexity:    24.89\n",
      "Epoch  25  1750/ 6203 batches: lr 0.04, loss  3.26, perplexity:    26.14\n",
      "Epoch  25  2000/ 6203 batches: lr 0.04, loss  3.27, perplexity:    26.31\n",
      "Epoch  25  2250/ 6203 batches: lr 0.04, loss  3.27, perplexity:    26.24\n",
      "Epoch  25  2500/ 6203 batches: lr 0.04, loss  3.19, perplexity:    24.35\n",
      "Epoch  25  2750/ 6203 batches: lr 0.04, loss  3.24, perplexity:    25.56\n",
      "Epoch  25  3000/ 6203 batches: lr 0.04, loss  3.30, perplexity:    27.05\n",
      "Epoch  25  3250/ 6203 batches: lr 0.04, loss  3.21, perplexity:    24.82\n",
      "Epoch  25  3500/ 6203 batches: lr 0.04, loss  3.20, perplexity:    24.41\n",
      "Epoch  25  3750/ 6203 batches: lr 0.04, loss  3.26, perplexity:    25.94\n",
      "Epoch  25  4000/ 6203 batches: lr 0.04, loss  3.23, perplexity:    25.26\n",
      "Epoch  25  4250/ 6203 batches: lr 0.04, loss  3.20, perplexity:    24.50\n",
      "Epoch  25  4500/ 6203 batches: lr 0.04, loss  3.18, perplexity:    24.12\n",
      "Epoch  25  4750/ 6203 batches: lr 0.04, loss  3.21, perplexity:    24.70\n",
      "Epoch  25  5000/ 6203 batches: lr 0.04, loss  3.18, perplexity:    24.09\n",
      "Epoch  25  5250/ 6203 batches: lr 0.04, loss  3.19, perplexity:    24.41\n",
      "Epoch  25  5500/ 6203 batches: lr 0.04, loss  3.21, perplexity:    24.67\n",
      "Epoch  25  5750/ 6203 batches: lr 0.04, loss  3.22, perplexity:    25.06\n",
      "Epoch  25  6000/ 6203 batches: lr 0.04, loss  3.22, perplexity:    25.08\n",
      "============================================================================\n",
      "Epoch  25 results: time: 453.99s, validation loss  3.60, perplexity    36.46\n",
      "============================================================================\n",
      "Epoch  26   250/ 6203 batches: lr 0.04, loss  3.24, perplexity:    25.56\n",
      "Epoch  26   500/ 6203 batches: lr 0.04, loss  3.24, perplexity:    25.65\n",
      "Epoch  26   750/ 6203 batches: lr 0.04, loss  3.24, perplexity:    25.43\n",
      "Epoch  26  1000/ 6203 batches: lr 0.04, loss  3.18, perplexity:    23.95\n",
      "Epoch  26  1250/ 6203 batches: lr 0.04, loss  3.21, perplexity:    24.82\n",
      "Epoch  26  1500/ 6203 batches: lr 0.04, loss  3.21, perplexity:    24.85\n",
      "Epoch  26  1750/ 6203 batches: lr 0.04, loss  3.26, perplexity:    26.08\n",
      "Epoch  26  2000/ 6203 batches: lr 0.04, loss  3.27, perplexity:    26.43\n",
      "Epoch  26  2250/ 6203 batches: lr 0.04, loss  3.26, perplexity:    26.16\n",
      "Epoch  26  2500/ 6203 batches: lr 0.04, loss  3.20, perplexity:    24.52\n",
      "Epoch  26  2750/ 6203 batches: lr 0.04, loss  3.24, perplexity:    25.51\n",
      "Epoch  26  3000/ 6203 batches: lr 0.04, loss  3.30, perplexity:    27.10\n",
      "Epoch  26  3250/ 6203 batches: lr 0.04, loss  3.21, perplexity:    24.74\n",
      "Epoch  26  3500/ 6203 batches: lr 0.04, loss  3.19, perplexity:    24.35\n",
      "Epoch  26  3750/ 6203 batches: lr 0.04, loss  3.25, perplexity:    25.87\n",
      "Epoch  26  4000/ 6203 batches: lr 0.04, loss  3.23, perplexity:    25.28\n",
      "Epoch  26  4250/ 6203 batches: lr 0.04, loss  3.20, perplexity:    24.44\n",
      "Epoch  26  4500/ 6203 batches: lr 0.04, loss  3.19, perplexity:    24.19\n",
      "Epoch  26  4750/ 6203 batches: lr 0.04, loss  3.20, perplexity:    24.58\n",
      "Epoch  26  5000/ 6203 batches: lr 0.04, loss  3.18, perplexity:    23.97\n",
      "Epoch  26  5250/ 6203 batches: lr 0.04, loss  3.20, perplexity:    24.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  26  5500/ 6203 batches: lr 0.04, loss  3.20, perplexity:    24.61\n",
      "Epoch  26  5750/ 6203 batches: lr 0.04, loss  3.23, perplexity:    25.18\n",
      "Epoch  26  6000/ 6203 batches: lr 0.04, loss  3.22, perplexity:    25.04\n",
      "============================================================================\n",
      "Epoch  26 results: time: 453.92s, validation loss  3.60, perplexity    36.45\n",
      "============================================================================\n",
      "Epoch  27   250/ 6203 batches: lr 0.04, loss  3.24, perplexity:    25.61\n",
      "Epoch  27   500/ 6203 batches: lr 0.04, loss  3.24, perplexity:    25.57\n",
      "Epoch  27   750/ 6203 batches: lr 0.04, loss  3.24, perplexity:    25.48\n",
      "Epoch  27  1000/ 6203 batches: lr 0.04, loss  3.18, perplexity:    23.93\n",
      "Epoch  27  1250/ 6203 batches: lr 0.04, loss  3.21, perplexity:    24.84\n",
      "Epoch  27  1500/ 6203 batches: lr 0.04, loss  3.21, perplexity:    24.67\n",
      "Epoch  27  1750/ 6203 batches: lr 0.04, loss  3.26, perplexity:    25.99\n",
      "Epoch  27  2000/ 6203 batches: lr 0.04, loss  3.27, perplexity:    26.42\n",
      "Epoch  27  2250/ 6203 batches: lr 0.04, loss  3.27, perplexity:    26.26\n",
      "Epoch  27  2500/ 6203 batches: lr 0.04, loss  3.20, perplexity:    24.53\n",
      "Epoch  27  2750/ 6203 batches: lr 0.04, loss  3.24, perplexity:    25.46\n",
      "Epoch  27  3000/ 6203 batches: lr 0.04, loss  3.30, perplexity:    27.04\n",
      "Epoch  27  3250/ 6203 batches: lr 0.04, loss  3.21, perplexity:    24.75\n",
      "Epoch  27  3500/ 6203 batches: lr 0.04, loss  3.19, perplexity:    24.39\n",
      "Epoch  27  3750/ 6203 batches: lr 0.04, loss  3.25, perplexity:    25.76\n",
      "Epoch  27  4000/ 6203 batches: lr 0.04, loss  3.23, perplexity:    25.28\n",
      "Epoch  27  4250/ 6203 batches: lr 0.04, loss  3.20, perplexity:    24.44\n",
      "Epoch  27  4500/ 6203 batches: lr 0.04, loss  3.18, perplexity:    24.14\n",
      "Epoch  27  4750/ 6203 batches: lr 0.04, loss  3.20, perplexity:    24.58\n",
      "Epoch  27  5000/ 6203 batches: lr 0.04, loss  3.18, perplexity:    24.04\n",
      "Epoch  27  5250/ 6203 batches: lr 0.04, loss  3.20, perplexity:    24.42\n",
      "Epoch  27  5500/ 6203 batches: lr 0.04, loss  3.20, perplexity:    24.56\n",
      "Epoch  27  5750/ 6203 batches: lr 0.04, loss  3.23, perplexity:    25.21\n",
      "Epoch  27  6000/ 6203 batches: lr 0.04, loss  3.22, perplexity:    25.11\n",
      "============================================================================\n",
      "Epoch  27 results: time: 454.84s, validation loss  3.60, perplexity    36.45\n",
      "============================================================================\n",
      "Epoch  28   250/ 6203 batches: lr 0.04, loss  3.24, perplexity:    25.66\n",
      "Epoch  28   500/ 6203 batches: lr 0.04, loss  3.24, perplexity:    25.52\n",
      "Epoch  28   750/ 6203 batches: lr 0.04, loss  3.23, perplexity:    25.35\n",
      "Epoch  28  1000/ 6203 batches: lr 0.04, loss  3.17, perplexity:    23.81\n",
      "Epoch  28  1250/ 6203 batches: lr 0.04, loss  3.21, perplexity:    24.79\n",
      "Epoch  28  1500/ 6203 batches: lr 0.04, loss  3.21, perplexity:    24.68\n",
      "Epoch  28  1750/ 6203 batches: lr 0.04, loss  3.26, perplexity:    26.05\n",
      "Epoch  28  2000/ 6203 batches: lr 0.04, loss  3.27, perplexity:    26.43\n",
      "Epoch  28  2250/ 6203 batches: lr 0.04, loss  3.27, perplexity:    26.25\n",
      "Epoch  28  2500/ 6203 batches: lr 0.04, loss  3.19, perplexity:    24.37\n",
      "Epoch  28  2750/ 6203 batches: lr 0.04, loss  3.24, perplexity:    25.60\n",
      "Epoch  28  3000/ 6203 batches: lr 0.04, loss  3.30, perplexity:    27.10\n",
      "Epoch  28  3250/ 6203 batches: lr 0.04, loss  3.21, perplexity:    24.85\n",
      "Epoch  28  3500/ 6203 batches: lr 0.04, loss  3.20, perplexity:    24.42\n",
      "Epoch  28  3750/ 6203 batches: lr 0.04, loss  3.25, perplexity:    25.80\n",
      "Epoch  28  4000/ 6203 batches: lr 0.04, loss  3.23, perplexity:    25.24\n",
      "Epoch  28  4250/ 6203 batches: lr 0.04, loss  3.20, perplexity:    24.54\n",
      "Epoch  28  4500/ 6203 batches: lr 0.04, loss  3.19, perplexity:    24.18\n",
      "Epoch  28  4750/ 6203 batches: lr 0.04, loss  3.21, perplexity:    24.70\n",
      "Epoch  28  5000/ 6203 batches: lr 0.04, loss  3.18, perplexity:    23.98\n",
      "Epoch  28  5250/ 6203 batches: lr 0.04, loss  3.20, perplexity:    24.42\n",
      "Epoch  28  5500/ 6203 batches: lr 0.04, loss  3.20, perplexity:    24.60\n",
      "Epoch  28  5750/ 6203 batches: lr 0.04, loss  3.22, perplexity:    25.11\n",
      "Epoch  28  6000/ 6203 batches: lr 0.04, loss  3.22, perplexity:    25.00\n",
      "============================================================================\n",
      "Epoch  28 results: time: 454.52s, validation loss  3.60, perplexity    36.44\n",
      "============================================================================\n",
      "Epoch  29   250/ 6203 batches: lr 0.04, loss  3.24, perplexity:    25.48\n",
      "Epoch  29   500/ 6203 batches: lr 0.04, loss  3.24, perplexity:    25.46\n",
      "Epoch  29   750/ 6203 batches: lr 0.04, loss  3.23, perplexity:    25.36\n",
      "Epoch  29  1000/ 6203 batches: lr 0.04, loss  3.17, perplexity:    23.88\n",
      "Epoch  29  1250/ 6203 batches: lr 0.04, loss  3.21, perplexity:    24.74\n",
      "Epoch  29  1500/ 6203 batches: lr 0.04, loss  3.20, perplexity:    24.64\n",
      "Epoch  29  1750/ 6203 batches: lr 0.04, loss  3.25, perplexity:    25.91\n",
      "Epoch  29  2000/ 6203 batches: lr 0.04, loss  3.28, perplexity:    26.45\n",
      "Epoch  29  2250/ 6203 batches: lr 0.04, loss  3.26, perplexity:    26.15\n",
      "Epoch  29  2500/ 6203 batches: lr 0.04, loss  3.20, perplexity:    24.41\n",
      "Epoch  29  2750/ 6203 batches: lr 0.04, loss  3.24, perplexity:    25.47\n",
      "Epoch  29  3000/ 6203 batches: lr 0.04, loss  3.29, perplexity:    26.86\n",
      "Epoch  29  3250/ 6203 batches: lr 0.04, loss  3.21, perplexity:    24.75\n",
      "Epoch  29  3500/ 6203 batches: lr 0.04, loss  3.20, perplexity:    24.42\n",
      "Epoch  29  3750/ 6203 batches: lr 0.04, loss  3.25, perplexity:    25.78\n",
      "Epoch  29  4000/ 6203 batches: lr 0.04, loss  3.23, perplexity:    25.23\n",
      "Epoch  29  4250/ 6203 batches: lr 0.04, loss  3.20, perplexity:    24.45\n",
      "Epoch  29  4500/ 6203 batches: lr 0.04, loss  3.19, perplexity:    24.18\n",
      "Epoch  29  4750/ 6203 batches: lr 0.04, loss  3.21, perplexity:    24.77\n",
      "Epoch  29  5000/ 6203 batches: lr 0.04, loss  3.18, perplexity:    23.98\n",
      "Epoch  29  5250/ 6203 batches: lr 0.04, loss  3.19, perplexity:    24.33\n",
      "Epoch  29  5500/ 6203 batches: lr 0.04, loss  3.20, perplexity:    24.42\n",
      "Epoch  29  5750/ 6203 batches: lr 0.04, loss  3.22, perplexity:    25.10\n",
      "Epoch  29  6000/ 6203 batches: lr 0.04, loss  3.22, perplexity:    24.96\n",
      "============================================================================\n",
      "Epoch  29 results: time: 454.65s, validation loss  3.60, perplexity    36.45\n",
      "============================================================================\n",
      "Epoch  30   250/ 6203 batches: lr 0.01, loss  3.24, perplexity:    25.57\n",
      "Epoch  30   500/ 6203 batches: lr 0.01, loss  3.23, perplexity:    25.38\n",
      "Epoch  30   750/ 6203 batches: lr 0.01, loss  3.24, perplexity:    25.46\n",
      "Epoch  30  1000/ 6203 batches: lr 0.01, loss  3.17, perplexity:    23.88\n",
      "Epoch  30  1250/ 6203 batches: lr 0.01, loss  3.21, perplexity:    24.82\n",
      "Epoch  30  1500/ 6203 batches: lr 0.01, loss  3.21, perplexity:    24.83\n",
      "Epoch  30  1750/ 6203 batches: lr 0.01, loss  3.26, perplexity:    26.00\n",
      "Epoch  30  2000/ 6203 batches: lr 0.01, loss  3.27, perplexity:    26.30\n",
      "Epoch  30  2250/ 6203 batches: lr 0.01, loss  3.26, perplexity:    26.15\n",
      "Epoch  30  2500/ 6203 batches: lr 0.01, loss  3.19, perplexity:    24.39\n",
      "Epoch  30  2750/ 6203 batches: lr 0.01, loss  3.24, perplexity:    25.53\n",
      "Epoch  30  3000/ 6203 batches: lr 0.01, loss  3.30, perplexity:    27.02\n",
      "Epoch  30  3250/ 6203 batches: lr 0.01, loss  3.21, perplexity:    24.70\n",
      "Epoch  30  3500/ 6203 batches: lr 0.01, loss  3.19, perplexity:    24.39\n",
      "Epoch  30  3750/ 6203 batches: lr 0.01, loss  3.25, perplexity:    25.88\n",
      "Epoch  30  4000/ 6203 batches: lr 0.01, loss  3.23, perplexity:    25.30\n",
      "Epoch  30  4250/ 6203 batches: lr 0.01, loss  3.19, perplexity:    24.39\n",
      "Epoch  30  4500/ 6203 batches: lr 0.01, loss  3.18, perplexity:    24.17\n",
      "Epoch  30  4750/ 6203 batches: lr 0.01, loss  3.21, perplexity:    24.72\n",
      "Epoch  30  5000/ 6203 batches: lr 0.01, loss  3.18, perplexity:    23.94\n",
      "Epoch  30  5250/ 6203 batches: lr 0.01, loss  3.19, perplexity:    24.36\n",
      "Epoch  30  5500/ 6203 batches: lr 0.01, loss  3.20, perplexity:    24.60\n",
      "Epoch  30  5750/ 6203 batches: lr 0.01, loss  3.22, perplexity:    25.05\n",
      "Epoch  30  6000/ 6203 batches: lr 0.01, loss  3.22, perplexity:    25.01\n",
      "============================================================================\n",
      "Epoch  30 results: time: 454.53s, validation loss  3.60, perplexity    36.44\n",
      "============================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  31   250/ 6203 batches: lr 0.01, loss  3.24, perplexity:    25.50\n",
      "Epoch  31   500/ 6203 batches: lr 0.01, loss  3.24, perplexity:    25.56\n",
      "Epoch  31   750/ 6203 batches: lr 0.01, loss  3.23, perplexity:    25.36\n",
      "Epoch  31  1000/ 6203 batches: lr 0.01, loss  3.17, perplexity:    23.85\n",
      "Epoch  31  1250/ 6203 batches: lr 0.01, loss  3.21, perplexity:    24.82\n",
      "Epoch  31  1500/ 6203 batches: lr 0.01, loss  3.20, perplexity:    24.59\n",
      "Epoch  31  1750/ 6203 batches: lr 0.01, loss  3.26, perplexity:    26.08\n",
      "Epoch  31  2000/ 6203 batches: lr 0.01, loss  3.27, perplexity:    26.28\n",
      "Epoch  31  2250/ 6203 batches: lr 0.01, loss  3.27, perplexity:    26.25\n",
      "Epoch  31  2500/ 6203 batches: lr 0.01, loss  3.19, perplexity:    24.25\n",
      "Epoch  31  2750/ 6203 batches: lr 0.01, loss  3.23, perplexity:    25.41\n",
      "Epoch  31  3000/ 6203 batches: lr 0.01, loss  3.29, perplexity:    26.97\n",
      "Epoch  31  3250/ 6203 batches: lr 0.01, loss  3.20, perplexity:    24.58\n",
      "Epoch  31  3500/ 6203 batches: lr 0.01, loss  3.19, perplexity:    24.27\n",
      "Epoch  31  3750/ 6203 batches: lr 0.01, loss  3.25, perplexity:    25.77\n",
      "Epoch  31  4000/ 6203 batches: lr 0.01, loss  3.23, perplexity:    25.16\n",
      "Epoch  31  4250/ 6203 batches: lr 0.01, loss  3.19, perplexity:    24.40\n",
      "Epoch  31  4500/ 6203 batches: lr 0.01, loss  3.19, perplexity:    24.25\n",
      "Epoch  31  4750/ 6203 batches: lr 0.01, loss  3.20, perplexity:    24.62\n",
      "Epoch  31  5000/ 6203 batches: lr 0.01, loss  3.18, perplexity:    23.99\n",
      "Epoch  31  5250/ 6203 batches: lr 0.01, loss  3.20, perplexity:    24.42\n",
      "Epoch  31  5500/ 6203 batches: lr 0.01, loss  3.19, perplexity:    24.39\n",
      "Epoch  31  5750/ 6203 batches: lr 0.01, loss  3.22, perplexity:    24.99\n",
      "Epoch  31  6000/ 6203 batches: lr 0.01, loss  3.21, perplexity:    24.90\n",
      "============================================================================\n",
      "Epoch  31 results: time: 454.64s, validation loss  3.60, perplexity    36.44\n",
      "============================================================================\n",
      "Epoch  32   250/ 6203 batches: lr 0.01, loss  3.24, perplexity:    25.61\n",
      "Epoch  32   500/ 6203 batches: lr 0.01, loss  3.24, perplexity:    25.56\n",
      "Epoch  32   750/ 6203 batches: lr 0.01, loss  3.23, perplexity:    25.37\n",
      "Epoch  32  1000/ 6203 batches: lr 0.01, loss  3.17, perplexity:    23.86\n",
      "Epoch  32  1250/ 6203 batches: lr 0.01, loss  3.21, perplexity:    24.72\n",
      "Epoch  32  1500/ 6203 batches: lr 0.01, loss  3.21, perplexity:    24.73\n",
      "Epoch  32  1750/ 6203 batches: lr 0.01, loss  3.25, perplexity:    25.83\n",
      "Epoch  32  2000/ 6203 batches: lr 0.01, loss  3.27, perplexity:    26.35\n",
      "Epoch  32  2250/ 6203 batches: lr 0.01, loss  3.26, perplexity:    26.10\n",
      "Epoch  32  2500/ 6203 batches: lr 0.01, loss  3.19, perplexity:    24.33\n",
      "Epoch  32  2750/ 6203 batches: lr 0.01, loss  3.24, perplexity:    25.46\n",
      "Epoch  32  3000/ 6203 batches: lr 0.01, loss  3.30, perplexity:    27.05\n",
      "Epoch  32  3250/ 6203 batches: lr 0.01, loss  3.21, perplexity:    24.71\n",
      "Epoch  32  3500/ 6203 batches: lr 0.01, loss  3.19, perplexity:    24.30\n",
      "Epoch  32  3750/ 6203 batches: lr 0.01, loss  3.25, perplexity:    25.68\n",
      "Epoch  32  4000/ 6203 batches: lr 0.01, loss  3.23, perplexity:    25.30\n",
      "Epoch  32  4250/ 6203 batches: lr 0.01, loss  3.20, perplexity:    24.48\n",
      "Epoch  32  4500/ 6203 batches: lr 0.01, loss  3.18, perplexity:    24.09\n",
      "Epoch  32  4750/ 6203 batches: lr 0.01, loss  3.20, perplexity:    24.62\n",
      "Epoch  32  5000/ 6203 batches: lr 0.01, loss  3.17, perplexity:    23.93\n",
      "Epoch  32  5250/ 6203 batches: lr 0.01, loss  3.19, perplexity:    24.37\n",
      "Epoch  32  5500/ 6203 batches: lr 0.01, loss  3.20, perplexity:    24.61\n",
      "Epoch  32  5750/ 6203 batches: lr 0.01, loss  3.22, perplexity:    25.10\n",
      "Epoch  32  6000/ 6203 batches: lr 0.01, loss  3.22, perplexity:    24.93\n",
      "============================================================================\n",
      "Epoch  32 results: time: 454.75s, validation loss  3.60, perplexity    36.43\n",
      "============================================================================\n",
      "Epoch  33   250/ 6203 batches: lr 0.01, loss  3.24, perplexity:    25.62\n",
      "Epoch  33   500/ 6203 batches: lr 0.01, loss  3.24, perplexity:    25.52\n",
      "Epoch  33   750/ 6203 batches: lr 0.01, loss  3.24, perplexity:    25.54\n",
      "Epoch  33  1000/ 6203 batches: lr 0.01, loss  3.17, perplexity:    23.83\n",
      "Epoch  33  1250/ 6203 batches: lr 0.01, loss  3.21, perplexity:    24.74\n",
      "Epoch  33  1500/ 6203 batches: lr 0.01, loss  3.20, perplexity:    24.65\n",
      "Epoch  33  1750/ 6203 batches: lr 0.01, loss  3.26, perplexity:    26.00\n",
      "Epoch  33  2000/ 6203 batches: lr 0.01, loss  3.27, perplexity:    26.27\n",
      "Epoch  33  2250/ 6203 batches: lr 0.01, loss  3.27, perplexity:    26.20\n",
      "Epoch  33  2500/ 6203 batches: lr 0.01, loss  3.19, perplexity:    24.36\n",
      "Epoch  33  2750/ 6203 batches: lr 0.01, loss  3.23, perplexity:    25.34\n",
      "Epoch  33  3000/ 6203 batches: lr 0.01, loss  3.29, perplexity:    26.96\n",
      "Epoch  33  3250/ 6203 batches: lr 0.01, loss  3.20, perplexity:    24.65\n",
      "Epoch  33  3500/ 6203 batches: lr 0.01, loss  3.19, perplexity:    24.35\n",
      "Epoch  33  3750/ 6203 batches: lr 0.01, loss  3.25, perplexity:    25.78\n",
      "Epoch  33  4000/ 6203 batches: lr 0.01, loss  3.23, perplexity:    25.18\n",
      "Epoch  33  4250/ 6203 batches: lr 0.01, loss  3.20, perplexity:    24.47\n",
      "Epoch  33  4500/ 6203 batches: lr 0.01, loss  3.18, perplexity:    24.11\n",
      "Epoch  33  4750/ 6203 batches: lr 0.01, loss  3.20, perplexity:    24.49\n",
      "Epoch  33  5000/ 6203 batches: lr 0.01, loss  3.18, perplexity:    23.95\n",
      "Epoch  33  5250/ 6203 batches: lr 0.01, loss  3.19, perplexity:    24.28\n",
      "Epoch  33  5500/ 6203 batches: lr 0.01, loss  3.20, perplexity:    24.53\n",
      "Epoch  33  5750/ 6203 batches: lr 0.01, loss  3.22, perplexity:    25.07\n",
      "Epoch  33  6000/ 6203 batches: lr 0.01, loss  3.22, perplexity:    25.05\n",
      "============================================================================\n",
      "Epoch  33 results: time: 454.59s, validation loss  3.60, perplexity    36.43\n",
      "============================================================================\n",
      "Epoch  34   250/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.42\n",
      "Epoch  34   500/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.36\n",
      "Epoch  34   750/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.39\n",
      "Epoch  34  1000/ 6203 batches: lr 0.00, loss  3.17, perplexity:    23.85\n",
      "Epoch  34  1250/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.81\n",
      "Epoch  34  1500/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.69\n",
      "Epoch  34  1750/ 6203 batches: lr 0.00, loss  3.26, perplexity:    25.98\n",
      "Epoch  34  2000/ 6203 batches: lr 0.00, loss  3.26, perplexity:    26.11\n",
      "Epoch  34  2250/ 6203 batches: lr 0.00, loss  3.27, perplexity:    26.24\n",
      "Epoch  34  2500/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.40\n",
      "Epoch  34  2750/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.52\n",
      "Epoch  34  3000/ 6203 batches: lr 0.00, loss  3.29, perplexity:    26.94\n",
      "Epoch  34  3250/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.65\n",
      "Epoch  34  3500/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.32\n",
      "Epoch  34  3750/ 6203 batches: lr 0.00, loss  3.25, perplexity:    25.77\n",
      "Epoch  34  4000/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.30\n",
      "Epoch  34  4250/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.47\n",
      "Epoch  34  4500/ 6203 batches: lr 0.00, loss  3.18, perplexity:    24.15\n",
      "Epoch  34  4750/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.62\n",
      "Epoch  34  5000/ 6203 batches: lr 0.00, loss  3.18, perplexity:    23.98\n",
      "Epoch  34  5250/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.35\n",
      "Epoch  34  5500/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.57\n",
      "Epoch  34  5750/ 6203 batches: lr 0.00, loss  3.22, perplexity:    24.98\n",
      "Epoch  34  6000/ 6203 batches: lr 0.00, loss  3.22, perplexity:    25.03\n",
      "============================================================================\n",
      "Epoch  34 results: time: 454.70s, validation loss  3.60, perplexity    36.43\n",
      "============================================================================\n",
      "Epoch  35   250/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.50\n",
      "Epoch  35   500/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.51\n",
      "Epoch  35   750/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.45\n",
      "Epoch  35  1000/ 6203 batches: lr 0.00, loss  3.18, perplexity:    23.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  35  1250/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.66\n",
      "Epoch  35  1500/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.76\n",
      "Epoch  35  1750/ 6203 batches: lr 0.00, loss  3.26, perplexity:    26.05\n",
      "Epoch  35  2000/ 6203 batches: lr 0.00, loss  3.27, perplexity:    26.23\n",
      "Epoch  35  2250/ 6203 batches: lr 0.00, loss  3.27, perplexity:    26.24\n",
      "Epoch  35  2500/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.45\n",
      "Epoch  35  2750/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.40\n",
      "Epoch  35  3000/ 6203 batches: lr 0.00, loss  3.29, perplexity:    26.97\n",
      "Epoch  35  3250/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.71\n",
      "Epoch  35  3500/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.38\n",
      "Epoch  35  3750/ 6203 batches: lr 0.00, loss  3.25, perplexity:    25.78\n",
      "Epoch  35  4000/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.22\n",
      "Epoch  35  4250/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.34\n",
      "Epoch  35  4500/ 6203 batches: lr 0.00, loss  3.18, perplexity:    24.15\n",
      "Epoch  35  4750/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.57\n",
      "Epoch  35  5000/ 6203 batches: lr 0.00, loss  3.17, perplexity:    23.93\n",
      "Epoch  35  5250/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.36\n",
      "Epoch  35  5500/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.46\n",
      "Epoch  35  5750/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.20\n",
      "Epoch  35  6000/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.87\n",
      "============================================================================\n",
      "Epoch  35 results: time: 454.61s, validation loss  3.60, perplexity    36.43\n",
      "============================================================================\n",
      "Epoch  36   250/ 6203 batches: lr 0.00, loss  3.25, perplexity:    25.70\n",
      "Epoch  36   500/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.53\n",
      "Epoch  36   750/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.36\n",
      "Epoch  36  1000/ 6203 batches: lr 0.00, loss  3.18, perplexity:    23.93\n",
      "Epoch  36  1250/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.73\n",
      "Epoch  36  1500/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.74\n",
      "Epoch  36  1750/ 6203 batches: lr 0.00, loss  3.26, perplexity:    25.99\n",
      "Epoch  36  2000/ 6203 batches: lr 0.00, loss  3.27, perplexity:    26.29\n",
      "Epoch  36  2250/ 6203 batches: lr 0.00, loss  3.26, perplexity:    26.18\n",
      "Epoch  36  2500/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.44\n",
      "Epoch  36  2750/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.45\n",
      "Epoch  36  3000/ 6203 batches: lr 0.00, loss  3.29, perplexity:    26.96\n",
      "Epoch  36  3250/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.65\n",
      "Epoch  36  3500/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.29\n",
      "Epoch  36  3750/ 6203 batches: lr 0.00, loss  3.25, perplexity:    25.71\n",
      "Epoch  36  4000/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.24\n",
      "Epoch  36  4250/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.36\n",
      "Epoch  36  4500/ 6203 batches: lr 0.00, loss  3.18, perplexity:    24.11\n",
      "Epoch  36  4750/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.52\n",
      "Epoch  36  5000/ 6203 batches: lr 0.00, loss  3.18, perplexity:    23.98\n",
      "Epoch  36  5250/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.43\n",
      "Epoch  36  5500/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.49\n",
      "Epoch  36  5750/ 6203 batches: lr 0.00, loss  3.22, perplexity:    25.14\n",
      "Epoch  36  6000/ 6203 batches: lr 0.00, loss  3.22, perplexity:    24.92\n",
      "============================================================================\n",
      "Epoch  36 results: time: 454.68s, validation loss  3.60, perplexity    36.43\n",
      "============================================================================\n",
      "Epoch  37   250/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.52\n",
      "Epoch  37   500/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.53\n",
      "Epoch  37   750/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.33\n",
      "Epoch  37  1000/ 6203 batches: lr 0.00, loss  3.17, perplexity:    23.77\n",
      "Epoch  37  1250/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.76\n",
      "Epoch  37  1500/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.78\n",
      "Epoch  37  1750/ 6203 batches: lr 0.00, loss  3.25, perplexity:    25.90\n",
      "Epoch  37  2000/ 6203 batches: lr 0.00, loss  3.26, perplexity:    26.18\n",
      "Epoch  37  2250/ 6203 batches: lr 0.00, loss  3.26, perplexity:    26.02\n",
      "Epoch  37  2500/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.40\n",
      "Epoch  37  2750/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.33\n",
      "Epoch  37  3000/ 6203 batches: lr 0.00, loss  3.29, perplexity:    26.89\n",
      "Epoch  37  3250/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.71\n",
      "Epoch  37  3500/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.32\n",
      "Epoch  37  3750/ 6203 batches: lr 0.00, loss  3.26, perplexity:    25.96\n",
      "Epoch  37  4000/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.18\n",
      "Epoch  37  4250/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.43\n",
      "Epoch  37  4500/ 6203 batches: lr 0.00, loss  3.18, perplexity:    24.15\n",
      "Epoch  37  4750/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.68\n",
      "Epoch  37  5000/ 6203 batches: lr 0.00, loss  3.18, perplexity:    23.96\n",
      "Epoch  37  5250/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.28\n",
      "Epoch  37  5500/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.52\n",
      "Epoch  37  5750/ 6203 batches: lr 0.00, loss  3.22, perplexity:    25.10\n",
      "Epoch  37  6000/ 6203 batches: lr 0.00, loss  3.22, perplexity:    24.98\n",
      "============================================================================\n",
      "Epoch  37 results: time: 454.74s, validation loss  3.60, perplexity    36.43\n",
      "============================================================================\n",
      "Epoch  38   250/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.60\n",
      "Epoch  38   500/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.36\n",
      "Epoch  38   750/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.43\n",
      "Epoch  38  1000/ 6203 batches: lr 0.00, loss  3.17, perplexity:    23.82\n",
      "Epoch  38  1250/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.78\n",
      "Epoch  38  1500/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.78\n",
      "Epoch  38  1750/ 6203 batches: lr 0.00, loss  3.26, perplexity:    25.97\n",
      "Epoch  38  2000/ 6203 batches: lr 0.00, loss  3.27, perplexity:    26.43\n",
      "Epoch  38  2250/ 6203 batches: lr 0.00, loss  3.26, perplexity:    26.16\n",
      "Epoch  38  2500/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.28\n",
      "Epoch  38  2750/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.46\n",
      "Epoch  38  3000/ 6203 batches: lr 0.00, loss  3.29, perplexity:    26.93\n",
      "Epoch  38  3250/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.59\n",
      "Epoch  38  3500/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.36\n",
      "Epoch  38  3750/ 6203 batches: lr 0.00, loss  3.25, perplexity:    25.72\n",
      "Epoch  38  4000/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.27\n",
      "Epoch  38  4250/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.33\n",
      "Epoch  38  4500/ 6203 batches: lr 0.00, loss  3.18, perplexity:    24.14\n",
      "Epoch  38  4750/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.67\n",
      "Epoch  38  5000/ 6203 batches: lr 0.00, loss  3.18, perplexity:    23.97\n",
      "Epoch  38  5250/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.42\n",
      "Epoch  38  5500/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.49\n",
      "Epoch  38  5750/ 6203 batches: lr 0.00, loss  3.22, perplexity:    25.03\n",
      "Epoch  38  6000/ 6203 batches: lr 0.00, loss  3.22, perplexity:    25.04\n",
      "============================================================================\n",
      "Epoch  38 results: time: 454.59s, validation loss  3.60, perplexity    36.43\n",
      "============================================================================\n",
      "Epoch  39   250/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.65\n",
      "Epoch  39   500/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.43\n",
      "Epoch  39   750/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.43\n",
      "Epoch  39  1000/ 6203 batches: lr 0.00, loss  3.18, perplexity:    23.98\n",
      "Epoch  39  1250/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.66\n",
      "Epoch  39  1500/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.71\n",
      "Epoch  39  1750/ 6203 batches: lr 0.00, loss  3.26, perplexity:    25.99\n",
      "Epoch  39  2000/ 6203 batches: lr 0.00, loss  3.27, perplexity:    26.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  39  2250/ 6203 batches: lr 0.00, loss  3.27, perplexity:    26.22\n",
      "Epoch  39  2500/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.36\n",
      "Epoch  39  2750/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.51\n",
      "Epoch  39  3000/ 6203 batches: lr 0.00, loss  3.29, perplexity:    26.87\n",
      "Epoch  39  3250/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.63\n",
      "Epoch  39  3500/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.39\n",
      "Epoch  39  3750/ 6203 batches: lr 0.00, loss  3.25, perplexity:    25.71\n",
      "Epoch  39  4000/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.18\n",
      "Epoch  39  4250/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.47\n",
      "Epoch  39  4500/ 6203 batches: lr 0.00, loss  3.18, perplexity:    24.10\n",
      "Epoch  39  4750/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.70\n",
      "Epoch  39  5000/ 6203 batches: lr 0.00, loss  3.18, perplexity:    24.02\n",
      "Epoch  39  5250/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.35\n",
      "Epoch  39  5500/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.58\n",
      "Epoch  39  5750/ 6203 batches: lr 0.00, loss  3.22, perplexity:    25.13\n",
      "Epoch  39  6000/ 6203 batches: lr 0.00, loss  3.22, perplexity:    25.00\n",
      "============================================================================\n",
      "Epoch  39 results: time: 454.69s, validation loss  3.60, perplexity    36.43\n",
      "============================================================================\n",
      "Epoch  40   250/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.54\n",
      "Epoch  40   500/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.45\n",
      "Epoch  40   750/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.53\n",
      "Epoch  40  1000/ 6203 batches: lr 0.00, loss  3.17, perplexity:    23.73\n",
      "Epoch  40  1250/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.76\n",
      "Epoch  40  1500/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.75\n",
      "Epoch  40  1750/ 6203 batches: lr 0.00, loss  3.25, perplexity:    25.92\n",
      "Epoch  40  2000/ 6203 batches: lr 0.00, loss  3.27, perplexity:    26.26\n",
      "Epoch  40  2250/ 6203 batches: lr 0.00, loss  3.27, perplexity:    26.22\n",
      "Epoch  40  2500/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.47\n",
      "Epoch  40  2750/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.35\n",
      "Epoch  40  3000/ 6203 batches: lr 0.00, loss  3.29, perplexity:    26.82\n",
      "Epoch  40  3250/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.68\n",
      "Epoch  40  3500/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.31\n",
      "Epoch  40  3750/ 6203 batches: lr 0.00, loss  3.25, perplexity:    25.80\n",
      "Epoch  40  4000/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.19\n",
      "Epoch  40  4250/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.42\n",
      "Epoch  40  4500/ 6203 batches: lr 0.00, loss  3.18, perplexity:    24.13\n",
      "Epoch  40  4750/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.70\n",
      "Epoch  40  5000/ 6203 batches: lr 0.00, loss  3.17, perplexity:    23.91\n",
      "Epoch  40  5250/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.36\n",
      "Epoch  40  5500/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.57\n",
      "Epoch  40  5750/ 6203 batches: lr 0.00, loss  3.22, perplexity:    25.09\n",
      "Epoch  40  6000/ 6203 batches: lr 0.00, loss  3.22, perplexity:    25.00\n",
      "============================================================================\n",
      "Epoch  40 results: time: 454.75s, validation loss  3.60, perplexity    36.43\n",
      "============================================================================\n",
      "Epoch  41   250/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.65\n",
      "Epoch  41   500/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.49\n",
      "Epoch  41   750/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.43\n",
      "Epoch  41  1000/ 6203 batches: lr 0.00, loss  3.17, perplexity:    23.79\n",
      "Epoch  41  1250/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.70\n",
      "Epoch  41  1500/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.68\n",
      "Epoch  41  1750/ 6203 batches: lr 0.00, loss  3.26, perplexity:    26.02\n",
      "Epoch  41  2000/ 6203 batches: lr 0.00, loss  3.27, perplexity:    26.26\n",
      "Epoch  41  2250/ 6203 batches: lr 0.00, loss  3.26, perplexity:    26.17\n",
      "Epoch  41  2500/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.38\n",
      "Epoch  41  2750/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.46\n",
      "Epoch  41  3000/ 6203 batches: lr 0.00, loss  3.30, perplexity:    26.99\n",
      "Epoch  41  3250/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.59\n",
      "Epoch  41  3500/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.35\n",
      "Epoch  41  3750/ 6203 batches: lr 0.00, loss  3.25, perplexity:    25.75\n",
      "Epoch  41  4000/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.16\n",
      "Epoch  41  4250/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.51\n",
      "Epoch  41  4500/ 6203 batches: lr 0.00, loss  3.18, perplexity:    24.17\n",
      "Epoch  41  4750/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.52\n",
      "Epoch  41  5000/ 6203 batches: lr 0.00, loss  3.17, perplexity:    23.87\n",
      "Epoch  41  5250/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.29\n",
      "Epoch  41  5500/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.47\n",
      "Epoch  41  5750/ 6203 batches: lr 0.00, loss  3.22, perplexity:    25.14\n",
      "Epoch  41  6000/ 6203 batches: lr 0.00, loss  3.22, perplexity:    25.07\n",
      "============================================================================\n",
      "Epoch  41 results: time: 454.97s, validation loss  3.60, perplexity    36.43\n",
      "============================================================================\n",
      "Epoch  42   250/ 6203 batches: lr 0.00, loss  3.25, perplexity:    25.72\n",
      "Epoch  42   500/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.56\n",
      "Epoch  42   750/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.45\n",
      "Epoch  42  1000/ 6203 batches: lr 0.00, loss  3.17, perplexity:    23.76\n",
      "Epoch  42  1250/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.74\n",
      "Epoch  42  1500/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.85\n",
      "Epoch  42  1750/ 6203 batches: lr 0.00, loss  3.26, perplexity:    25.94\n",
      "Epoch  42  2000/ 6203 batches: lr 0.00, loss  3.27, perplexity:    26.31\n",
      "Epoch  42  2250/ 6203 batches: lr 0.00, loss  3.26, perplexity:    26.13\n",
      "Epoch  42  2500/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.53\n",
      "Epoch  42  2750/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.49\n",
      "Epoch  42  3000/ 6203 batches: lr 0.00, loss  3.29, perplexity:    26.87\n",
      "Epoch  42  3250/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.66\n",
      "Epoch  42  3500/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.26\n",
      "Epoch  42  3750/ 6203 batches: lr 0.00, loss  3.26, perplexity:    25.92\n",
      "Epoch  42  4000/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.26\n",
      "Epoch  42  4250/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.45\n",
      "Epoch  42  4500/ 6203 batches: lr 0.00, loss  3.18, perplexity:    24.07\n",
      "Epoch  42  4750/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.63\n",
      "Epoch  42  5000/ 6203 batches: lr 0.00, loss  3.18, perplexity:    23.95\n",
      "Epoch  42  5250/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.31\n",
      "Epoch  42  5500/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.45\n",
      "Epoch  42  5750/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.20\n",
      "Epoch  42  6000/ 6203 batches: lr 0.00, loss  3.22, perplexity:    24.96\n",
      "============================================================================\n",
      "Epoch  42 results: time: 454.62s, validation loss  3.60, perplexity    36.43\n",
      "============================================================================\n",
      "Epoch  43   250/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.57\n",
      "Epoch  43   500/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.59\n",
      "Epoch  43   750/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.39\n",
      "Epoch  43  1000/ 6203 batches: lr 0.00, loss  3.17, perplexity:    23.86\n",
      "Epoch  43  1250/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.70\n",
      "Epoch  43  1500/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.73\n",
      "Epoch  43  1750/ 6203 batches: lr 0.00, loss  3.25, perplexity:    25.84\n",
      "Epoch  43  2000/ 6203 batches: lr 0.00, loss  3.27, perplexity:    26.32\n",
      "Epoch  43  2250/ 6203 batches: lr 0.00, loss  3.26, perplexity:    26.18\n",
      "Epoch  43  2500/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.33\n",
      "Epoch  43  2750/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.49\n",
      "Epoch  43  3000/ 6203 batches: lr 0.00, loss  3.29, perplexity:    26.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  43  3250/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.70\n",
      "Epoch  43  3500/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.24\n",
      "Epoch  43  3750/ 6203 batches: lr 0.00, loss  3.25, perplexity:    25.83\n",
      "Epoch  43  4000/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.24\n",
      "Epoch  43  4250/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.36\n",
      "Epoch  43  4500/ 6203 batches: lr 0.00, loss  3.18, perplexity:    24.04\n",
      "Epoch  43  4750/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.66\n",
      "Epoch  43  5000/ 6203 batches: lr 0.00, loss  3.18, perplexity:    23.93\n",
      "Epoch  43  5250/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.37\n",
      "Epoch  43  5500/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.54\n",
      "Epoch  43  5750/ 6203 batches: lr 0.00, loss  3.22, perplexity:    25.14\n",
      "Epoch  43  6000/ 6203 batches: lr 0.00, loss  3.22, perplexity:    25.05\n",
      "============================================================================\n",
      "Epoch  43 results: time: 454.72s, validation loss  3.60, perplexity    36.43\n",
      "============================================================================\n",
      "Epoch  44   250/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.57\n",
      "Epoch  44   500/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.51\n",
      "Epoch  44   750/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.37\n",
      "Epoch  44  1000/ 6203 batches: lr 0.00, loss  3.17, perplexity:    23.78\n",
      "Epoch  44  1250/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.70\n",
      "Epoch  44  1500/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.79\n",
      "Epoch  44  1750/ 6203 batches: lr 0.00, loss  3.26, perplexity:    25.97\n",
      "Epoch  44  2000/ 6203 batches: lr 0.00, loss  3.27, perplexity:    26.29\n",
      "Epoch  44  2250/ 6203 batches: lr 0.00, loss  3.27, perplexity:    26.19\n",
      "Epoch  44  2500/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.34\n",
      "Epoch  44  2750/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.43\n",
      "Epoch  44  3000/ 6203 batches: lr 0.00, loss  3.29, perplexity:    26.91\n",
      "Epoch  44  3250/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.63\n",
      "Epoch  44  3500/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.35\n",
      "Epoch  44  3750/ 6203 batches: lr 0.00, loss  3.25, perplexity:    25.81\n",
      "Epoch  44  4000/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.23\n",
      "Epoch  44  4250/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.44\n",
      "Epoch  44  4500/ 6203 batches: lr 0.00, loss  3.18, perplexity:    24.11\n",
      "Epoch  44  4750/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.62\n",
      "Epoch  44  5000/ 6203 batches: lr 0.00, loss  3.18, perplexity:    24.06\n",
      "Epoch  44  5250/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.38\n",
      "Epoch  44  5500/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.48\n",
      "Epoch  44  5750/ 6203 batches: lr 0.00, loss  3.22, perplexity:    25.14\n",
      "Epoch  44  6000/ 6203 batches: lr 0.00, loss  3.22, perplexity:    25.00\n",
      "============================================================================\n",
      "Epoch  44 results: time: 454.75s, validation loss  3.60, perplexity    36.43\n",
      "============================================================================\n",
      "Epoch  45   250/ 6203 batches: lr 0.00, loss  3.25, perplexity:    25.69\n",
      "Epoch  45   500/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.42\n",
      "Epoch  45   750/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.46\n",
      "Epoch  45  1000/ 6203 batches: lr 0.00, loss  3.17, perplexity:    23.87\n",
      "Epoch  45  1250/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.80\n",
      "Epoch  45  1500/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.78\n",
      "Epoch  45  1750/ 6203 batches: lr 0.00, loss  3.26, perplexity:    26.01\n",
      "Epoch  45  2000/ 6203 batches: lr 0.00, loss  3.27, perplexity:    26.35\n",
      "Epoch  45  2250/ 6203 batches: lr 0.00, loss  3.26, perplexity:    26.12\n",
      "Epoch  45  2500/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.42\n",
      "Epoch  45  2750/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.50\n",
      "Epoch  45  3000/ 6203 batches: lr 0.00, loss  3.29, perplexity:    26.97\n",
      "Epoch  45  3250/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.70\n",
      "Epoch  45  3500/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.27\n",
      "Epoch  45  3750/ 6203 batches: lr 0.00, loss  3.25, perplexity:    25.69\n",
      "Epoch  45  4000/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.26\n",
      "Epoch  45  4250/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.38\n",
      "Epoch  45  4500/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.18\n",
      "Epoch  45  4750/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.59\n",
      "Epoch  45  5000/ 6203 batches: lr 0.00, loss  3.18, perplexity:    24.12\n",
      "Epoch  45  5250/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.30\n",
      "Epoch  45  5500/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.46\n",
      "Epoch  45  5750/ 6203 batches: lr 0.00, loss  3.22, perplexity:    25.02\n",
      "Epoch  45  6000/ 6203 batches: lr 0.00, loss  3.22, perplexity:    25.07\n",
      "============================================================================\n",
      "Epoch  45 results: time: 454.72s, validation loss  3.60, perplexity    36.43\n",
      "============================================================================\n",
      "Epoch  46   250/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.60\n",
      "Epoch  46   500/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.54\n",
      "Epoch  46   750/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.34\n",
      "Epoch  46  1000/ 6203 batches: lr 0.00, loss  3.17, perplexity:    23.85\n",
      "Epoch  46  1250/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.70\n",
      "Epoch  46  1500/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.68\n",
      "Epoch  46  1750/ 6203 batches: lr 0.00, loss  3.25, perplexity:    25.79\n",
      "Epoch  46  2000/ 6203 batches: lr 0.00, loss  3.27, perplexity:    26.37\n",
      "Epoch  46  2250/ 6203 batches: lr 0.00, loss  3.27, perplexity:    26.20\n",
      "Epoch  46  2500/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.38\n",
      "Epoch  46  2750/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.51\n",
      "Epoch  46  3000/ 6203 batches: lr 0.00, loss  3.29, perplexity:    26.93\n",
      "Epoch  46  3250/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.67\n",
      "Epoch  46  3500/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.44\n",
      "Epoch  46  3750/ 6203 batches: lr 0.00, loss  3.25, perplexity:    25.84\n",
      "Epoch  46  4000/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.23\n",
      "Epoch  46  4250/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.40\n",
      "Epoch  46  4500/ 6203 batches: lr 0.00, loss  3.18, perplexity:    24.05\n",
      "Epoch  46  4750/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.59\n",
      "Epoch  46  5000/ 6203 batches: lr 0.00, loss  3.18, perplexity:    23.93\n",
      "Epoch  46  5250/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.42\n",
      "Epoch  46  5500/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.51\n",
      "Epoch  46  5750/ 6203 batches: lr 0.00, loss  3.22, perplexity:    25.05\n",
      "Epoch  46  6000/ 6203 batches: lr 0.00, loss  3.22, perplexity:    25.07\n",
      "============================================================================\n",
      "Epoch  46 results: time: 454.83s, validation loss  3.60, perplexity    36.43\n",
      "============================================================================\n",
      "Epoch  47   250/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.52\n",
      "Epoch  47   500/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.47\n",
      "Epoch  47   750/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.38\n",
      "Epoch  47  1000/ 6203 batches: lr 0.00, loss  3.17, perplexity:    23.90\n",
      "Epoch  47  1250/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.74\n",
      "Epoch  47  1500/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.64\n",
      "Epoch  47  1750/ 6203 batches: lr 0.00, loss  3.26, perplexity:    25.98\n",
      "Epoch  47  2000/ 6203 batches: lr 0.00, loss  3.27, perplexity:    26.25\n",
      "Epoch  47  2250/ 6203 batches: lr 0.00, loss  3.26, perplexity:    26.14\n",
      "Epoch  47  2500/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.28\n",
      "Epoch  47  2750/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.52\n",
      "Epoch  47  3000/ 6203 batches: lr 0.00, loss  3.30, perplexity:    27.11\n",
      "Epoch  47  3250/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.58\n",
      "Epoch  47  3500/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.34\n",
      "Epoch  47  3750/ 6203 batches: lr 0.00, loss  3.25, perplexity:    25.90\n",
      "Epoch  47  4000/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  47  4250/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.34\n",
      "Epoch  47  4500/ 6203 batches: lr 0.00, loss  3.18, perplexity:    24.12\n",
      "Epoch  47  4750/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.65\n",
      "Epoch  47  5000/ 6203 batches: lr 0.00, loss  3.18, perplexity:    23.96\n",
      "Epoch  47  5250/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.38\n",
      "Epoch  47  5500/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.55\n",
      "Epoch  47  5750/ 6203 batches: lr 0.00, loss  3.22, perplexity:    25.11\n",
      "Epoch  47  6000/ 6203 batches: lr 0.00, loss  3.22, perplexity:    25.05\n",
      "============================================================================\n",
      "Epoch  47 results: time: 454.89s, validation loss  3.60, perplexity    36.43\n",
      "============================================================================\n",
      "Epoch  48   250/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.56\n",
      "Epoch  48   500/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.57\n",
      "Epoch  48   750/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.30\n",
      "Epoch  48  1000/ 6203 batches: lr 0.00, loss  3.17, perplexity:    23.82\n",
      "Epoch  48  1250/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.70\n",
      "Epoch  48  1500/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.63\n",
      "Epoch  48  1750/ 6203 batches: lr 0.00, loss  3.26, perplexity:    25.94\n",
      "Epoch  48  2000/ 6203 batches: lr 0.00, loss  3.27, perplexity:    26.26\n",
      "Epoch  48  2250/ 6203 batches: lr 0.00, loss  3.27, perplexity:    26.21\n",
      "Epoch  48  2500/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.44\n",
      "Epoch  48  2750/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.34\n",
      "Epoch  48  3000/ 6203 batches: lr 0.00, loss  3.29, perplexity:    26.92\n",
      "Epoch  48  3250/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.74\n",
      "Epoch  48  3500/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.25\n",
      "Epoch  48  3750/ 6203 batches: lr 0.00, loss  3.25, perplexity:    25.82\n",
      "Epoch  48  4000/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.34\n",
      "Epoch  48  4250/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.43\n",
      "Epoch  48  4500/ 6203 batches: lr 0.00, loss  3.18, perplexity:    24.14\n",
      "Epoch  48  4750/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.58\n",
      "Epoch  48  5000/ 6203 batches: lr 0.00, loss  3.17, perplexity:    23.90\n",
      "Epoch  48  5250/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.31\n",
      "Epoch  48  5500/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.59\n",
      "Epoch  48  5750/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.18\n",
      "Epoch  48  6000/ 6203 batches: lr 0.00, loss  3.22, perplexity:    24.95\n",
      "============================================================================\n",
      "Epoch  48 results: time: 454.74s, validation loss  3.60, perplexity    36.43\n",
      "============================================================================\n",
      "Epoch  49   250/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.46\n",
      "Epoch  49   500/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.57\n",
      "Epoch  49   750/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.49\n",
      "Epoch  49  1000/ 6203 batches: lr 0.00, loss  3.18, perplexity:    23.95\n",
      "Epoch  49  1250/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.75\n",
      "Epoch  49  1500/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.82\n",
      "Epoch  49  1750/ 6203 batches: lr 0.00, loss  3.26, perplexity:    25.98\n",
      "Epoch  49  2000/ 6203 batches: lr 0.00, loss  3.27, perplexity:    26.33\n",
      "Epoch  49  2250/ 6203 batches: lr 0.00, loss  3.26, perplexity:    26.11\n",
      "Epoch  49  2500/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.40\n",
      "Epoch  49  2750/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.43\n",
      "Epoch  49  3000/ 6203 batches: lr 0.00, loss  3.29, perplexity:    26.94\n",
      "Epoch  49  3250/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.71\n",
      "Epoch  49  3500/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.35\n",
      "Epoch  49  3750/ 6203 batches: lr 0.00, loss  3.25, perplexity:    25.87\n",
      "Epoch  49  4000/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.25\n",
      "Epoch  49  4250/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.34\n",
      "Epoch  49  4500/ 6203 batches: lr 0.00, loss  3.18, perplexity:    24.14\n",
      "Epoch  49  4750/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.62\n",
      "Epoch  49  5000/ 6203 batches: lr 0.00, loss  3.18, perplexity:    23.99\n",
      "Epoch  49  5250/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.31\n",
      "Epoch  49  5500/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.61\n",
      "Epoch  49  5750/ 6203 batches: lr 0.00, loss  3.22, perplexity:    25.10\n",
      "Epoch  49  6000/ 6203 batches: lr 0.00, loss  3.22, perplexity:    24.92\n",
      "============================================================================\n",
      "Epoch  49 results: time: 454.57s, validation loss  3.60, perplexity    36.43\n",
      "============================================================================\n",
      "Epoch  50   250/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.58\n",
      "Epoch  50   500/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.64\n",
      "Epoch  50   750/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.44\n",
      "Epoch  50  1000/ 6203 batches: lr 0.00, loss  3.17, perplexity:    23.85\n",
      "Epoch  50  1250/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.72\n",
      "Epoch  50  1500/ 6203 batches: lr 0.00, loss  3.21, perplexity:    24.79\n",
      "Epoch  50  1750/ 6203 batches: lr 0.00, loss  3.26, perplexity:    25.97\n",
      "Epoch  50  2000/ 6203 batches: lr 0.00, loss  3.27, perplexity:    26.30\n",
      "Epoch  50  2250/ 6203 batches: lr 0.00, loss  3.27, perplexity:    26.22\n",
      "Epoch  50  2500/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.36\n",
      "Epoch  50  2750/ 6203 batches: lr 0.00, loss  3.24, perplexity:    25.47\n",
      "Epoch  50  3000/ 6203 batches: lr 0.00, loss  3.30, perplexity:    26.99\n",
      "Epoch  50  3250/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.61\n",
      "Epoch  50  3500/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.36\n",
      "Epoch  50  3750/ 6203 batches: lr 0.00, loss  3.25, perplexity:    25.78\n",
      "Epoch  50  4000/ 6203 batches: lr 0.00, loss  3.23, perplexity:    25.31\n",
      "Epoch  50  4250/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.39\n",
      "Epoch  50  4500/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.18\n",
      "Epoch  50  4750/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.65\n",
      "Epoch  50  5000/ 6203 batches: lr 0.00, loss  3.17, perplexity:    23.92\n",
      "Epoch  50  5250/ 6203 batches: lr 0.00, loss  3.19, perplexity:    24.32\n",
      "Epoch  50  5500/ 6203 batches: lr 0.00, loss  3.20, perplexity:    24.45\n",
      "Epoch  50  5750/ 6203 batches: lr 0.00, loss  3.22, perplexity:    25.10\n",
      "Epoch  50  6000/ 6203 batches: lr 0.00, loss  3.22, perplexity:    25.10\n",
      "============================================================================\n",
      "Epoch  50 results: time: 454.98s, validation loss  3.60, perplexity    36.43\n",
      "============================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyson/anaconda3/envs/ml/lib/python3.6/site-packages/ipykernel/__main__.py:41: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "/n End of training. Test loss:  3.63, Test perplexity:    37.88\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "# At any point we can hit Ctrl+C to break out of training early\n",
    "try:\n",
    "    train()\n",
    "except KeyboardInterrupt:\n",
    "    print('='*75)\n",
    "    print('Exiting from training early')\n",
    "    \n",
    "# Load the best saved model\n",
    "with open('model_checkpoint.pt', 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "\n",
    "# Run on the test data\n",
    "test_loss = evaluate(test_data)\n",
    "print('='*75)\n",
    "print('/n End of training. Test loss: {:5.2f}, Test perplexity: {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)\n",
    "))\n",
    "print('='*75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sample Output\n",
    "Now that we've trained our model, we can generate new sentences from our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_seed = 423\n",
    "num_output_words = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 0/500 words\n",
      "Generated 250/500 words\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(random_seed)\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "\n",
    "with open('model_checkpoint.pt', 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "    model.cuda()\n",
    "else:\n",
    "    model.cpu()\n",
    "\n",
    "corpus = Corpus(pathname)\n",
    "n_tokens = len(corpus.dictionary)\n",
    "hidden = model.init_hidden(1)\n",
    "input = Variable(torch.rand(1, 1).mul(n_tokens).long(), volatile=True)\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "    input.data = input.data.cuda()\n",
    "\n",
    "def get_token_map():\n",
    "    return {v: k for k, v in token_lookup().items()}\n",
    "\n",
    "with open('output.txt', 'w') as outf:\n",
    "    for i in range(num_output_words):\n",
    "        output, hidden = model(input, hidden)\n",
    "        word_weights = output.squeeze().data.exp().cpu()\n",
    "        word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "        input.data.fill_(word_idx)\n",
    "        word = corpus.dictionary.idx_to_word[word_idx]\n",
    "        \n",
    "        token_map = get_token_map()\n",
    "        is_token = False\n",
    "        \n",
    "        if token_map.get(word):\n",
    "            is_token = True\n",
    "            word = token_map[word]\n",
    "            \n",
    "        outf.write(('' if is_token else ' ') + word)\n",
    "        \n",
    "        if i % logging_interval == 0:\n",
    "            print('Generated {}/{} words'.format(i, num_output_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", doctor.\n",
      "\n",
      " crowd:( concerned gasp)\n",
      "\n",
      "( springfield mall: int. springfield mall- airport)\n",
      "\n",
      " reporter #1: catch, nelson, our only score our beatles are as big as a mess of a tiara!\n",
      "\n",
      " martin prince: i reckon we' ve been asleep.\n",
      "\n",
      " miss hoover: not for chief wiggum, you have changed the pudding super- cheeked! i got big and case you' re the husband!( snaps fingers)\n",
      "\n",
      " homer simpson: really? well, they got sex and worthwhile.\n",
      "\n",
      " lenny leonard: yeah, i' m lookin' for me.\n",
      "\n",
      " lenny leonard: oh, hi.\n",
      "\n",
      " homer simpson: you know if emotion must be in the water scrupulous mae?\n",
      "\n",
      " moe szyslak: ah, hello, family shootout.\n",
      "\n",
      " moe szyslak: ah, you' re not worse than i mean, lou. that' s what i' m thinking of--\n",
      "\n",
      " barney gumble:( pained sounds)( fading quietly) don' t worry. we have to be out. here they are... whoever' s all our names. we' ll be thankful for all your communist!\n",
      "\n",
      " grampa simpson: stop pushing snowball! i' ve got to talk to that right.\n",
      "\n",
      " lenny leonard:( trying to stop homer) you merely get him a tandem bike awwww.\n",
      "\n",
      " lenny leonard: what a rib.\n",
      "\n",
      " ned flanders: how' s he doing, flanders?\n",
      "\n",
      " ned flanders: well, nice thoughts, dude.\n",
      "\n",
      " lawyer: good, sir.\n",
      "\n",
      " homer simpson: where are you goji?! man, it' s going to the moment of the u. b. ad.\n",
      "\n",
      " carl carlson: we' re on ba- two, pinsky:... if you don' t be quite my dad, you can' t get a baby, so to proctor. roll up.\n",
      "\n",
      " ned flanders: well, eeyah, we broke her... it used to be a seacoast.\n",
      "\n",
      "( springfield streets: ext. springfield streets- night- establishing)\n",
      "\n",
      " blue- haired lawyer: knoxville, sr.... is capital city with techron rodan.\n",
      "\n",
      " homer simpson: woo hoo!\n",
      "\n",
      " lisa simpson: oh, it' s class, lisa.\n",
      "\n",
      " c. montgomery burns: and join our power, i can be to you.\n",
      "\n",
      " homer simpson: conta?!\n",
      "\n",
      "( simpson home: int. simpson house- kitchen- day)\n",
      "\n",
      " marge simpson:( to kang) i said it' s a bad idea."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat ./output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
